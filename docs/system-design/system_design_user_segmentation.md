# System Design: User Segmentation Analyzer (Personalized Productivity Interventions)

**Hypothesis:** Hypothesis 3.1 - Distinct "Power Users" vs "Casual Users" Enable Personalization
**Priority:** â­â­â­â­ **ACTIONABLE**
**ROI:** License optimization + personalized interventions = $500k-1M/year for 1,000 employees

**Purpose:** Design production system to segment users by engagement level and deliver personalized productivity interventions
**Focus:** Agentic workflow that identifies power/regular/casual users and provides tailored recommendations at Fortune 1000 scale

---

## ğŸ¯ Executive Summary

**Problem:** Organizations lack visibility into how employees engage with their tool stack. Power users need advanced features and integrations, while casual users struggle with complexity. One-size-fits-all IT policies create friction for both groups.

**Solution:** User Segmentation Analyzer - an agentic workflow that segments users by engagement level (power, regular, casual) and provides personalized productivity recommendations and license optimization insights.

**Key Metrics:**

- **User segmentation:** Power (50%), Regular (10%), Casual (40%) in sample data
- **License optimization:** Reclaim licenses from casual users â†’ $200k/year savings
- **Personalization:** Power users get advanced workflows, casual users get simplified UX
- **Adoption:** Improve tool adoption from 40% to 70% (casual â†’ regular)

**Sample Data Evidence:**

- 10 users segmented: 5 power (2 apps), 1 regular (1 app), 4 casual (0 apps)
- Wide engagement variability indicates opportunity for personalization
- 40% of users in auth-only events (potential license optimization opportunity)

---

## 1. Business Requirements

### Problem Statement

Fortune 1000 companies invest millions in software licenses and IT infrastructure, but lack visibility into how employees actually use these tools:

**Pain Points:**

- **Power users** (frequent, multi-tool users) need advanced features but IT doesn't know who they are
- **Casual users** (infrequent, single-tool users) are overwhelmed by complex UX designed for power users
- **Inactive users** (auth-only, no app access) hold expensive licenses that could be reallocated
- **IT teams** lack data to make informed decisions about training, licenses, or tool consolidation

**Sample Data Evidence:**

```
From 18 events, 10 users, 13-second window:
â”œâ”€â”€ Power users: 5 users (50%) - accessing 2 apps (max in sample)
â”œâ”€â”€ Regular users: 1 user (10%) - accessing 1 app
â””â”€â”€ Casual users: 4 users (40%) - accessing 0 apps (auth-only events)

Note: 13-second sample cannot distinguish true casual users from "not yet accessed" users.
Production data over days/weeks would show clearer patterns.

Typical production patterns (extrapolated):
â”œâ”€â”€ Power users (20-30%): 10+ apps/month, daily access, multi-role workflows
â”œâ”€â”€ Regular users (40-50%): 3-5 apps/month, 3-4x/week access, focused workflows
â””â”€â”€ Casual users (20-30%): 0-2 apps/month, infrequent access, potential churn
```

### CEO Questions Answered

1. **"How can we use AI to make teams 100x productive?"**
   â†’ Personalized interventions: Power users get workflow automation, casual users get simplified onboarding

2. **"Where is the waste?"**
   â†’ Casual users (40% in sample) may hold $200k/year in unused licenses

3. **"Where can we automate?"**
   â†’ Auto-detect power users for advanced training, auto-flag casual users for license review

### ROI Calculation

**Current State (No Segmentation):**

```
Generic IT Policies:
â”œâ”€â”€ All users get same training (wasteful for power users, overwhelming for casual)
â”œâ”€â”€ All users get same licenses (many casual users underutilize expensive tools)
â”œâ”€â”€ No personalization (power users plateau, casual users churn)
â””â”€â”€ IT spends time manually identifying who needs what

Example: 1,000-employee organization
â”œâ”€â”€ 400 casual users Ã— $500/year/license = $200k in underutilized licenses
â”œâ”€â”€ 300 power users Ã— 5 hrs/month lost to manual workarounds = $750k/year productivity loss
â””â”€â”€ Total waste: ~$1M/year
```

**Optimized State (With Segmentation):**

```
Personalized Interventions:
â”œâ”€â”€ Power users (300):
â”‚   â””â”€â”€ Advanced workflow bundles, API access, custom integrations
â”‚   â””â”€â”€ Time savings: 5 hrs/month Ã— $50/hr Ã— 300 = $75k/month = $900k/year
â”œâ”€â”€ Regular users (500):
â”‚   â””â”€â”€ Standard tools, guided workflows, proactive support
â”‚   â””â”€â”€ Maintain current productivity (no regression)
â””â”€â”€ Casual users (200):
    â”œâ”€â”€ Simplified onboarding, mobile-first UX, "essentials" plan
    â”œâ”€â”€ License reallocation: 100 licenses reclaimed Ã— $2k/year = $200k/year savings
    â””â”€â”€ Convert 50% to regular users (improved adoption) = additional $100k/year

Total ROI: $900k (power user efficiency) + $200k (license reclaim) = $1.1M/year
Conservative estimate: $500k-750k/year (accounting for implementation costs)
```

### CEO-Level Decisions Enabled

**Before User Segmentation:**

- "We need better tool adoption" (vague, no actionable data)
- IT guesses which users need training (manual, inefficient)
- Licenses purchased based on headcount (wasteful)

**After User Segmentation:**

- "200 casual users hold $200k in Salesforce licenses but access <1x/month â†’ migrate to lower tier"
- "300 power users waste 5 hrs/month on manual workflows â†’ invest in API integrations for this cohort"
- "50% of new hires remain casual after 90 days â†’ redesign onboarding for this segment"

---

## 2. Agentic Workflow Architecture

### Why Agentic > Traditional ML

| Aspect             | Traditional ML (K-means Clustering)                 | User Segmentation Analyzer (Agentic)                                                                                                                    |
| ------------------ | --------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Output**         | "User in cluster 2"                                 | "Power User (Multi-tool Specialist) BECAUSE accesses 12 apps daily with 85% of time in 3 core tools..."                                                 |
| **Explainability** | Black box - no reasoning                            | Natural language explanation: "Classified as power user based on: 450 app accesses/month, 12 unique apps, 95% login success rate"                       |
| **Adaptability**   | Requires retraining when engagement patterns change | Adapts to new engagement patterns (e.g., remote work â†’ different access patterns)                                                                       |
| **Actionable**     | "Cluster 2 has 300 users"                           | "300 power users waste 5 hrs/month on manual integrations. Recommendation: Enable API access for this cohort. ROI: $900k/year."                         |
| **IT-friendly**    | Requires data scientist to interpret                | IT manager can understand: "User is casual because: 2 app accesses in 90 days, last login 45 days ago"                                                  |
| **Example**        | "User engagement score: 0.32"                       | "Casual User (At-risk Churn). Evidence: 95% drop in activity over 60 days. Recommendation: Send re-engagement email with simplified quick-start guide." |

### Agent Design

```
User Segmentation Analyzer Agent
â””â”€â”€ Input: user_id
â””â”€â”€ Tools:
    â”œâ”€â”€ query_user_engagement_metrics(user_id, days=90)
    â”‚   â””â”€â”€ Returns: {
    â”‚         total_app_accesses: 450,
    â”‚         unique_apps: 12,
    â”‚         access_frequency_per_week: 18,
    â”‚         avg_session_duration_min: 45,
    â”‚         last_access_days_ago: 1,
    â”‚         auth_success_rate: 0.95,
    â”‚         device_count: 2,
    â”‚         location_diversity: 3
    â”‚       }
    â”œâ”€â”€ calculate_engagement_score(metrics)
    â”‚   â””â”€â”€ Returns: {
    â”‚         engagement_score: 0.85,  // 0-1 scale
    â”‚         percentile: 92,  // vs other users
    â”‚         trend: "stable"  // increasing/stable/decreasing
    â”‚       }
    â”œâ”€â”€ segment_user(engagement_metrics, engagement_score) [LLM]
    â”‚   â””â”€â”€ Input: User engagement metrics + organization context
    â”‚   â””â”€â”€ Output: Segment (power/regular/casual) + confidence + reasoning
    â”œâ”€â”€ detect_engagement_anomalies(user_id, historical_data)
    â”‚   â””â”€â”€ Returns: {
    â”‚         churn_risk: "high",  // user was power, now casual
    â”‚         rapid_growth: false,  // user went from casual to power
    â”‚         dormant: false  // no activity in 30+ days
    â”‚       }
    â””â”€â”€ recommend_interventions(segment, user_metrics, org_context) [LLM]
        â””â”€â”€ Input: User segment + engagement patterns + org policies
        â””â”€â”€ Output: Personalized recommendations

â””â”€â”€ Flow:
    1. Query BigQuery for user's engagement metrics (last 90 days):
       - Total app accesses (frequency)
       - Unique apps accessed (diversity)
       - Access frequency per week (consistency)
       - Session duration patterns (depth of engagement)
       - Recency (last access)
       - Device/location diversity (flexibility)

    2. Calculate engagement score (composite metric):
       engagement_score = (
           frequency_score Ã— 0.3 +      // How often (450 accesses â†’ high)
           diversity_score Ã— 0.2 +      // How many apps (12 apps â†’ high)
           recency_score Ã— 0.2 +        // How recent (1 day ago â†’ high)
           consistency_score Ã— 0.15 +   // Weekly pattern (18/week â†’ high)
           depth_score Ã— 0.15          // Session duration (45 min â†’ high)
       )
       Example: (0.9Ã—0.3) + (0.8Ã—0.2) + (1.0Ã—0.2) + (0.85Ã—0.15) + (0.75Ã—0.15) = 0.865

    3. LLM analyzes with structured reasoning:
       Prompt: "Given engagement score 0.865 (92nd percentile), classify user
               as power/regular/casual. Metrics:
               - 450 app accesses in 90 days (15/day avg)
               - 12 unique apps (top quartile)
               - Last access: 1 day ago (highly active)
               - Weekly frequency: 18 accesses/week (consistent)
               - Session duration: 45 min avg (deep engagement)

               Organization context:
               - Avg engagement score: 0.45
               - Power user threshold: >75th percentile (score >0.65)
               - Casual user threshold: <25th percentile (score <0.25)

               Classify with reasoning and confidence."

    4. LLM responds with structured output:
       {
         "segment": "Power User",
         "sub_segment": "Multi-tool Specialist",
         "confidence": 0.94,
         "reasoning": "User demonstrates exceptional engagement across
                      all dimensions. 450 app accesses in 90 days places
                      user in top 10% of organization. 12 unique apps
                      indicates multi-role workflows requiring diverse
                      toolset. Daily access pattern (18/week) shows
                      consistent, habitual usage. 45-minute avg sessions
                      indicate deep work, not just quick logins. User is
                      a quintessential power user.",
         "engagement_score": 0.865,
         "percentile_rank": 92,
         "evidence": [
           "Frequency: 450 accesses in 90 days (top 8% of org)",
           "Diversity: 12 unique apps (vs org avg of 3.5)",
           "Recency: Last access 1 day ago (highly active)",
           "Consistency: 18 accesses/week for 12 weeks (no gaps)",
           "Depth: 45-min avg session (vs org avg of 12 min)"
         ],
         "trend": "stable",
         "churn_risk": "very_low"
       }

    5. Detect anomalies (cheap SQL + simple rules):
       - Churn risk: Was power user (score >0.7) for 60 days, now <0.3 for 30 days
       - Rapid growth: Was casual (score <0.3) for 60 days, now >0.7 for 30 days
       - Dormant: No activity in 30+ days (recency score = 0)

    6. Generate personalized recommendations (2nd LLM call):
       Prompt: "User is Power User (Multi-tool Specialist) with 12 apps.
               Recommend productivity optimizations.

               User patterns:
               - Heavy user of: Slack, GitHub, Jira, Figma, Notion, Gmail
               - Moderate user of: Calendar, Drive, Analytics, AWS Console
               - Light user of: Zoom, Linear

               Current IT policies:
               - All users get basic training (1 hour onboarding)
               - All users on standard auth (step 6)
               - No workflow automation available

               Task: Recommend personalized interventions to improve
                     productivity for this power user segment."

       LLM responds:
       {
         "recommendations": [
           {
             "category": "Workflow Automation",
             "suggestion": "Enable API access + Zapier/Make.com integration",
             "reasoning": "User accesses 12 apps daily. Automating repetitive
                          cross-app workflows (e.g., Jira â†’ Slack notifications,
                          GitHub PR â†’ Linear ticket) could save 5-10 hrs/month.",
             "estimated_time_savings_hours_month": 7.5,
             "priority": "high",
             "implementation_cost": "$50/month (Zapier Pro)"
           },
           {
             "category": "Authentication",
             "suggestion": "Reduce auth steps from 6 â†’ 2 for trusted devices",
             "reasoning": "Power user accesses 18x/week with 95% success rate.
                          Auth friction wastes ~30 min/week. Risk is low given
                          established device trust.",
             "estimated_time_savings_hours_month": 2,
             "priority": "medium",
             "implementation_cost": "$0 (policy change)"
           },
           {
             "category": "Advanced Training",
             "suggestion": "Offer advanced certification program",
             "reasoning": "User already proficient with 12 tools. Advanced
                          training on API usage, automation, and shortcuts
                          could unlock 10x productivity for complex workflows.",
             "estimated_time_savings_hours_month": 10,
             "priority": "high",
             "implementation_cost": "$200/user (training program)"
           }
         ],
         "total_monthly_time_savings": 19.5,
         "annual_cost_savings": "$9,750/year for this user",
         "implementation_cost_annual": "$800/year"
       }

    7. Store segmentation + recommendations in BigQuery
    8. Update materialized view for CEO dashboard

â””â”€â”€ Output to User (via dashboard notification):
    "You're a Power User! ğŸš€
     You're in the top 10% of engaged users at your organization.

     We've unlocked advanced features for you:
     âœ“ API access enabled
     âœ“ Reduced auth steps (faster login)
     âœ“ Invited to Advanced User Community

     Estimated time savings: 19.5 hours/month"

â””â”€â”€ Output to IT Dashboard:
    "User Segmentation Summary (1,000 employees):

     ğŸ“Š Distribution:
     â€¢ Power Users (300, 30%): Top quartile engagement
     â€¢ Regular Users (500, 50%): Median engagement
     â€¢ Casual Users (200, 20%): Bottom quartile engagement

     ğŸ’° Opportunities:

     1. Power User Optimization (300 users):
        â€¢ Enable API access + workflow automation
        â€¢ Reduce auth friction
        â€¢ Advanced training program
        â€¢ Total savings: $2.9M/year (300 Ã— $9.75k)
        â€¢ Implementation cost: $240k/year
        â€¢ Net ROI: $2.66M/year

     2. License Optimization (200 casual users):
        â€¢ 100 users with 0 app accesses in 90 days
        â€¢ Hold $200k in unused licenses (Salesforce, Adobe, etc.)
        â€¢ Recommendation: Reclaim or migrate to lower tier
        â€¢ Savings: $200k/year

     3. Casual User Conversion (100 users):
        â€¢ 100 casual users could become regular with better onboarding
        â€¢ Simplified UX, guided workflows, mobile-first
        â€¢ Potential productivity gain: $100k/year

     Total Opportunity: $2.96M/year
     Priority: Start with license optimization (quick win, $200k)"
```

### Actual LLM Prompt (Memorize for Interview)

```
System: You are an enterprise productivity expert specializing in user engagement analysis and personalized recommendations.

User: Analyze the following employee's engagement metrics from the last 90 days and classify their user segment.

Engagement Metrics:
- Total app accesses: 450
- Unique apps accessed: 12
- Access frequency: 18 accesses/week (highly consistent)
- Average session duration: 45 minutes
- Last access: 1 day ago
- Authentication success rate: 95%
- Devices used: 2 (laptop + mobile)
- Locations: 3 (office, home, coffee shop)
- Engagement score: 0.865 (composite metric, 0-1 scale)

Organization Context:
- Total employees: 1,000
- Average engagement score: 0.45
- Power user threshold: >0.65 (top 25%)
- Regular user threshold: 0.25 - 0.65 (middle 50%)
- Casual user threshold: <0.25 (bottom 25%)
- User's percentile rank: 92nd percentile

Engagement Trend:
- Last 30 days: 0.88 (stable, slight increase)
- 30-60 days ago: 0.85
- 60-90 days ago: 0.83

Task:
1. Classify user segment: Power User, Regular User, or Casual User
2. Provide sub-segment if applicable (e.g., "Multi-tool Specialist", "Single-tool Expert", "At-risk Churn")
3. Calculate confidence score (0.0 to 1.0)
4. Explain reasoning with specific citations to engagement metrics
5. List supporting evidence
6. Assess churn risk (very_low, low, medium, high)

Output format (JSON):
{
  "segment": "Power User",
  "sub_segment": "Multi-tool Specialist",
  "confidence": 0.94,
  "reasoning": "User demonstrates exceptional engagement across all dimensions. 450 app accesses in 90 days (15 per day) places user in top 10% of organization. Access to 12 unique apps indicates multi-role workflows requiring diverse toolset, far exceeding org average of 3.5 apps. Consistent weekly pattern (18 accesses/week for 12 consecutive weeks) shows habitual, essential usage rather than sporadic experimentation. 45-minute average session duration indicates deep work and meaningful engagement, not just quick logins. Recency of 1 day confirms user is currently active. This is a quintessential power user.",
  "engagement_score": 0.865,
  "percentile_rank": 92,
  "evidence": [
    "Frequency: 450 accesses in 90 days, top 8% of organization",
    "Diversity: 12 unique apps (vs org avg of 3.5 apps)",
    "Recency: Last access 1 day ago, highly active",
    "Consistency: 18 accesses/week sustained for 12 weeks, no gaps",
    "Depth: 45-min avg session vs org avg of 12 min",
    "Multi-device: Uses laptop + mobile (flexibility)",
    "Multi-location: Office + home + mobile (adaptability)"
  ],
  "trend": "stable_slight_increase",
  "trend_explanation": "Engagement score increased from 0.83 to 0.88 over 90 days, indicating growing proficiency and tool adoption. No signs of decline.",
  "churn_risk": "very_low",
  "churn_risk_reasoning": "User shows no signs of disengagement. Consistent usage pattern, recent activity, and upward trend all indicate low churn risk. User is deeply embedded in workflow."
}
```

**Why this prompt works:**

- Provides comprehensive engagement context (LLM can reason about multiple dimensions)
- Includes organization benchmarks (percentile ranks, thresholds)
- Requests structured output for programmatic parsing
- Asks for trend analysis (is engagement increasing/decreasing?)
- Requires confidence scoring + evidence (explainability)
- Assesses churn risk (proactive intervention)

---

## 3. Data Pipeline (GCP Architecture)

### Ingestion (Same as Work Categorizer)

```
SSO Providers (Okta, Google Workspace, Azure AD)
    â†“ Webhooks (real-time) OR API Polling (every 30 sec)
Cloud Pub/Sub Topic: "sso-events-raw"
    â†“ Subscribe
Cloud Run Service: "sso-event-processor"
    â†“ Parse, validate, enrich
BigQuery Table: "sso_events_raw"
    â”œâ”€â”€ Partitioned by: date (YYYYMMDD)
    â”œâ”€â”€ Clustered by: actor_id, target_id
    â””â”€â”€ Row-level security: tenant_id (single-tenant isolation)
```

### Processing: Engagement Score Computation (Nightly Batch)

```
Cloud Scheduler (cron: 0 1 * * *)  # Run at 1 AM daily
    â†“ Trigger
Cloud Run Job: "engagement-score-calculator"
    â”œâ”€â”€ For each active user (last access within 90 days):
    â”‚   â”œâ”€â”€ Query BigQuery for 90-day activity:
    â”‚   â”‚   â”œâ”€â”€ COUNT(*) as total_app_accesses
    â”‚   â”‚   â”œâ”€â”€ COUNT(DISTINCT target_id) as unique_apps
    â”‚   â”‚   â”œâ”€â”€ AVG(TIMESTAMP_DIFF(events)) as avg_session_duration
    â”‚   â”‚   â”œâ”€â”€ MAX(published) as last_access
    â”‚   â”‚   â””â”€â”€ SUM(CASE WHEN outcome = 'SUCCESS') / COUNT(*) as success_rate
    â”‚   â”œâ”€â”€ Calculate engagement score (cheap SQL, no LLM):
    â”‚   â”‚   â””â”€â”€ engagement_score = weighted_average(frequency, diversity, recency, etc.)
    â”‚   â”œâ”€â”€ Calculate percentile rank (WHERE clause on same tenant):
    â”‚   â”‚   â””â”€â”€ percentile = PERCENT_RANK() OVER (ORDER BY engagement_score)
    â”‚   â””â”€â”€ Store in BigQuery table: "user_engagement_scores"
    â”‚       Schema: user_id, engagement_score, percentile, total_accesses,
    â”‚               unique_apps, last_access, trend, last_updated
    â”œâ”€â”€ Runtime: ~15 minutes for 10k users (simple SQL aggregations)
    â””â”€â”€ Cost: $2/day (BigQuery compute, no LLM calls)

Note: Engagement scores computed nightly (cheap SQL), not real-time
```

### Processing: User Segmentation (Weekly Batch with LLM)

```
Cloud Scheduler (cron: 0 2 * * 1)  # Run at 2 AM every Monday
    â†“ Trigger
Cloud Run Job: "user-segmentation-batch"
    â”œâ”€â”€ Query users needing re-segmentation:
    â”‚   â”œâ”€â”€ Engagement score changed >0.15 in last week (significant shift)
    â”‚   â”œâ”€â”€ OR 90 days since last segmentation (scheduled refresh)
    â”‚   â”œâ”€â”€ OR manager flagged for review
    â”‚   â””â”€â”€ Result: ~20% of users per week (2,000 out of 10k)
    â”œâ”€â”€ For each user:
    â”‚   â”œâ”€â”€ Fetch engagement metrics from BigQuery
    â”‚   â”œâ”€â”€ Call User Segmentation Analyzer Agent (2 LLM calls):
    â”‚   â”‚   â”œâ”€â”€ LLM call 1: segment_user (300 tokens)
    â”‚   â”‚   â””â”€â”€ LLM call 2: recommend_interventions (400 tokens)
    â”‚   â”œâ”€â”€ Cost: ~$0.012 per user (700 tokens Ã— $15/million)
    â”‚   â””â”€â”€ Store in "user_segments" table
    â”‚       Schema: user_id, segment, sub_segment, confidence, reasoning,
    â”‚               recommendations, engagement_score, percentile, last_updated
    â”œâ”€â”€ Runtime: ~1 hour for 2,000 users (LLM calls parallelized)
    â””â”€â”€ Total cost: 2,000 Ã— $0.012 = $24/week = $96/month

Note: Only re-segment users with significant changes (not all 10k every week)
```

### Storage

```
BigQuery Dataset: "user_engagement_insights"
â”œâ”€â”€ Table: "sso_events_raw"
â”‚   â””â”€â”€ Shared with Work Categorizer (same SSO data)
â”‚   â””â”€â”€ ~100 bytes/event, 1M events/day Ã— 365 = 36.5 GB/year
â”‚
â”œâ”€â”€ Table: "user_engagement_scores"
â”‚   â”œâ”€â”€ Schema: user_id, engagement_score, percentile, total_accesses,
â”‚   â”‚           unique_apps, last_access_days_ago, trend, last_updated
â”‚   â”œâ”€â”€ Rows: 10k users (all active users)
â”‚   â”œâ”€â”€ Refreshed: Nightly (cheap SQL, $2/day)
â”‚   â””â”€â”€ Cost: Negligible (~1 MB)
â”‚
â”œâ”€â”€ Table: "user_segments"
â”‚   â”œâ”€â”€ Schema: user_id, segment, sub_segment, confidence, reasoning,
â”‚   â”‚           recommendations, engagement_score, percentile, churn_risk,
â”‚   â”‚           last_updated
â”‚   â”œâ”€â”€ Rows: 10k users (all users, refreshed weekly for 20% with changes)
â”‚   â””â”€â”€ Cost: Negligible (~5 MB)
â”‚
â””â”€â”€ Materialized View: "user_segmentation_dashboard"
    â”œâ”€â”€ Pre-aggregates:
    â”‚   â”œâ”€â”€ Segment distribution (power: 30%, regular: 50%, casual: 20%)
    â”‚   â”œâ”€â”€ License optimization opportunities (casual users with premium licenses)
    â”‚   â”œâ”€â”€ Churn risk users (power â†’ casual transition)
    â”‚   â””â”€â”€ ROI calculations (time savings, license reclaim)
    â”œâ”€â”€ Refreshed every 6 hours
    â””â”€â”€ Dashboard load time: < 2 seconds
```

---

## 4. RBAC & Privacy

### Access Control Matrix

| Role         | Data Access                                             | Example Query Restriction                                                                    |
| ------------ | ------------------------------------------------------- | -------------------------------------------------------------------------------------------- |
| **CEO**      | Org-wide segment distribution (no individual users)     | `SELECT segment, COUNT(*) FROM user_segments WHERE tenant_id = X GROUP BY segment`           |
| **Manager**  | Direct reports' segments + recommendations              | `WHERE user_id IN (SELECT employee_id FROM org_hierarchy WHERE manager_id = SESSION_USER())` |
| **IT/HR**    | Full access to segments + license optimization insights | All data with audit trail                                                                    |
| **Employee** | Own segment + personalized recommendations              | `WHERE user_id = SESSION_USER()`                                                             |
| **Finance**  | License optimization data (aggregated, no PII)          | Can see "200 casual users hold $200k in licenses" but not individual names                   |

### BigQuery Row-Level Security (RLS)

```sql
-- CEO policy: Only aggregates
CREATE ROW ACCESS POLICY ceo_aggregate_only
ON user_engagement_insights.user_segments
GRANT TO ('group:ceos@company.com')
FILTER USING (FALSE);  -- CEOs query materialized view, not raw table

-- Manager policy: Direct reports only
CREATE ROW ACCESS POLICY manager_team_filter
ON user_engagement_insights.user_segments
GRANT TO ('group:managers@company.com')
FILTER USING (
    user_id IN (
        SELECT employee_id
        FROM user_engagement_insights.org_hierarchy
        WHERE manager_id = SESSION_USER()
    )
);

-- IT/HR policy: Full access with audit logging
CREATE ROW ACCESS POLICY it_hr_full_access
ON user_engagement_insights.user_segments
GRANT TO ('group:it-hr@company.com')
FILTER USING (TRUE);
-- All queries logged via BigQuery audit logs

-- Employee policy: Own data only
CREATE ROW ACCESS POLICY employee_self_service
ON user_engagement_insights.user_segments
GRANT TO ('group:employees@company.com')
FILTER USING (user_id = SESSION_USER());
```

### PII Handling & Privacy

```
Sensitive Fields:
â”œâ”€â”€ user_id (employee ID)
â”‚   â””â”€â”€ Hash with SHA-256 + per-tenant salt
â”œâ”€â”€ Engagement metrics (app access counts, session duration)
â”‚   â””â”€â”€ NOT PII, but can reveal work patterns
â”‚   â””â”€â”€ CEO sees aggregates only: "30% are power users"
â”‚   â””â”€â”€ Managers see team members: "Alice is power user, Bob is casual"
â”œâ”€â”€ Recommendations
â”‚   â””â”€â”€ NOT PII, but personalized to user
â”‚   â””â”€â”€ Employees see their own recommendations
â”‚   â””â”€â”€ Managers see team recommendations (for coaching)
â””â”€â”€ Churn risk flags
    â””â”€â”€ Sensitive! Manager can see "Alice at churn risk" to intervene

Data Retention:
â”œâ”€â”€ Engagement scores: 30-day rolling window (refreshed nightly)
â”œâ”€â”€ User segments: 365 days (track changes over time)
â”œâ”€â”€ Recommendations: 90 days (refresh quarterly)
â””â”€â”€ Aggregated metrics: 7 years (compliance, trend analysis)

GDPR Right-to-be-Forgotten:
â”œâ”€â”€ Delete user from:
â”‚   â”œâ”€â”€ user_engagement_scores
â”‚   â”œâ”€â”€ user_segments
â”‚   â””â”€â”€ sso_events_raw (shared table)
â”œâ”€â”€ Recalculate segment distribution excluding deleted user
â””â”€â”€ Audit log deletion request fulfillment
```

---

## 5. Evaluation & Quality

### Ground Truth Labels (Manager Validation)

```
Manager Labels 200 Users:
â”œâ”€â”€ Managers review each direct report's:
â”‚   â”œâ”€â”€ App usage patterns over 90 days
â”‚   â”œâ”€â”€ Work output quality (project completion, sprint velocity)
â”‚   â”œâ”€â”€ Tool proficiency (feedback from IT support tickets)
â”‚   â””â”€â”€ Engagement in training/community
â”œâ”€â”€ Label each user as:
â”‚   â”œâ”€â”€ "Power User" (highly engaged, multi-tool, drives adoption)
â”‚   â”œâ”€â”€ "Regular User" (consistent, focused, meets expectations)
â”‚   â””â”€â”€ "Casual User" (infrequent, underutilized, potential churn)
â””â”€â”€ Compare agent segments to manager labels

Evaluation Pipeline:
â”œâ”€â”€ Run User Segmentation Analyzer on 200 labeled users
â”œâ”€â”€ Compare agent segment vs manager label
â”œâ”€â”€ Accuracy = % of matches (Target: >85%)
â”œâ”€â”€ Confusion matrix:
â”‚   â”œâ”€â”€ False positives (agent says power, manager says casual)
â”‚   â”‚   â†’ Investigate: Is agent over-weighting metrics?
â”‚   â””â”€â”€ False negatives (agent says casual, manager says power)
â”‚       â†’ Investigate: Are we missing engagement signals?
â””â”€â”€ Iterate on engagement score formula to improve accuracy
```

### Segmentation Validation (Behavioral Cohort Analysis)

```
Validate segments predict future behavior:

Test: Do power users remain power users?
â”œâ”€â”€ Take users classified as "power" in Month 1
â”œâ”€â”€ Measure engagement in Month 2-3
â”œâ”€â”€ Expect: >80% remain in top quartile (segment is stable)
â””â”€â”€ If <70% remain â†’ segment is too volatile, tune thresholds

Test: Do casual users churn?
â”œâ”€â”€ Take users classified as "casual" in Month 1
â”œâ”€â”€ Measure % who become inactive (0 accesses) in Month 2-3
â”œâ”€â”€ Expect: 30-40% churn (validates casual = at-risk)
â””â”€â”€ If <20% churn â†’ casual threshold too broad, tighten

Test: Do interventions work?
â”œâ”€â”€ Power users given API access â†’ measure time savings
â”œâ”€â”€ Casual users given simplified UX â†’ measure conversion to regular
â”œâ”€â”€ Target: 20% of casual â†’ regular after intervention
â””â”€â”€ If <10% convert â†’ recommendations not actionable, revise
```

### Recommendation Effectiveness (A/B Test)

```
Experiment: Personalized Interventions Impact on Productivity
â”œâ”€â”€ Duration: 3 months
â”œâ”€â”€ Groups:
â”‚   â”œâ”€â”€ Control (500 employees):
â”‚   â”‚   â””â”€â”€ Generic IT policies, no personalization
â”‚   â””â”€â”€ Treatment (500 employees):
â”‚       â””â”€â”€ Personalized interventions based on segment:
â”‚           â”œâ”€â”€ Power users: API access, reduced auth, advanced training
â”‚           â”œâ”€â”€ Regular users: Guided workflows, proactive support
â”‚           â””â”€â”€ Casual users: Simplified UX, mobile-first, re-engagement
â”œâ”€â”€ Metrics:
â”‚   â”œâ”€â”€ Engagement score change (treatment should increase >10%)
â”‚   â”œâ”€â”€ License utilization (casual users in treatment should increase >20%)
â”‚   â”œâ”€â”€ Time savings (power users in treatment should report 5+ hrs/month)
â”‚   â””â”€â”€ User satisfaction (survey: "Are tools easier to use?" 1-5 scale)
â”œâ”€â”€ Success Criteria:
â”‚   â”œâ”€â”€ Power users: +5 hrs/month productivity (>$250/month value)
â”‚   â”œâ”€â”€ Regular users: Maintain current productivity (no regression)
â”‚   â””â”€â”€ Casual users: 20% convert to regular (improved engagement)
â””â”€â”€ ROI Validation:
    â””â”€â”€ If treatment group shows $500k/year productivity gain:
        â†’ Full rollout justified
```

### Success Metrics

| Metric                    | Target                                           | Measurement                              |
| ------------------------- | ------------------------------------------------ | ---------------------------------------- |
| **Segmentation Accuracy** | >85% match with manager labels                   | Compare to 200 labeled users             |
| **Segment Stability**     | >80% of power users remain power in next quarter | Cohort retention analysis                |
| **Churn Prediction**      | 30-40% of casual users churn (validates risk)    | Track casual â†’ inactive transition       |
| **License Optimization**  | Reclaim $200k/year in unused licenses            | Count casual users with premium licenses |
| **Conversion Rate**       | 20% of casual â†’ regular after intervention       | A/B test conversion metric               |
| **Time Savings**          | Power users save 5+ hrs/month with interventions | Survey + activity logs                   |
| **Cost**                  | <$100/month LLM costs for 10k employees          | BigQuery + LLM billing                   |

---

## 6. Tracing & Monitoring

### LLM Call Tracing (Every Segmentation Logged)

```json
Every LLM call logged to BigQuery table: "user_segmentation_llm_traces"
{
  "trace_id": "seg_abc123",
  "timestamp": "2025-01-15T02:30:00Z",
  "agent": "user_segmentation_analyzer",
  "user_id": "hash_xyz789",
  "tenant_id": "fortune1000_client_42",

  "llm_calls": [
    {
      "call_id": "llm_1",
      "model": "claude-sonnet-4.5",
      "purpose": "segment_user",
      "prompt_tokens": 320,
      "completion_tokens": 280,
      "cost_usd": 0.009,
      "latency_ms": 1450,
      "prompt_version": "user_seg_v1.2",
      "temperature": 0.2,
      "max_tokens": 350
    },
    {
      "call_id": "llm_2",
      "model": "claude-sonnet-4.5",
      "purpose": "recommend_interventions",
      "prompt_tokens": 380,
      "completion_tokens": 420,
      "cost_usd": 0.012,
      "latency_ms": 1620,
      "prompt_version": "recommendations_v1.0",
      "temperature": 0.3,
      "max_tokens": 500
    }
  ],

  "total_cost_usd": 0.021,
  "total_latency_ms": 3070,

  "input_context": {
    "engagement_score": 0.865,
    "percentile": 92,
    "total_accesses": 450,
    "unique_apps": 12,
    "last_access_days_ago": 1,
    "trend": "stable"
  },

  "output": {
    "segment": "Power User",
    "sub_segment": "Multi-tool Specialist",
    "confidence": 0.94,
    "churn_risk": "very_low",
    "recommendations_count": 3,
    "estimated_monthly_time_savings": 19.5
  },

  "error": null
}
```

### Monitoring Dashboards (Grafana + BigQuery)

```
1. User Segmentation Dashboard (CEO/IT-facing)
   â”œâ”€â”€ Segment distribution: Power 30%, Regular 50%, Casual 20%
   â”œâ”€â”€ Trend over time: Are casual users decreasing? (good)
   â”œâ”€â”€ License optimization opportunity: $200k in reclaimed licenses
   â”œâ”€â”€ Churn risk users: 50 power users â†’ casual in last 30 days (investigate)
   â””â”€â”€ ROI summary: $1.1M/year savings from personalization

2. LLM Cost & Performance Dashboard (Engineering)
   â”œâ”€â”€ Total LLM spend: $96/month (user segmentation)
   â”œâ”€â”€ Cost per segmentation: $0.012 avg
   â”œâ”€â”€ p50, p95, p99 latency: 1.5s, 3.2s, 4.8s
   â”œâ”€â”€ Error rate: <1%
   â””â”€â”€ Alert: If daily cost > $5 (budget overrun)

3. Engagement Score Dashboard (IT/HR)
   â”œâ”€â”€ Engagement score distribution (histogram)
   â”œâ”€â”€ Percentile thresholds (25th, 50th, 75th)
   â”œâ”€â”€ Trend analysis: Org avg engagement increasing? (good signal)
   â”œâ”€â”€ Anomalies: Users with rapid engagement drop (churn risk)
   â””â”€â”€ Alert: If org avg engagement drops >10% in one month

4. Intervention Effectiveness Dashboard (Product)
   â”œâ”€â”€ Power user interventions: % who report time savings
   â”œâ”€â”€ Casual user conversion: % who become regular
   â”œâ”€â”€ License optimization: $ reclaimed per quarter
   â”œâ”€â”€ User satisfaction: Avg rating (1-5 scale)
   â””â”€â”€ Alert: If satisfaction <3.5

5. Segmentation Quality Dashboard (ML Ops)
   â”œâ”€â”€ Accuracy vs manager labels (>85% target)
   â”œâ”€â”€ Confidence score distribution (avoid all 0.99 or all 0.5)
   â”œâ”€â”€ Segment stability: % of users who change segments each week
   â”œâ”€â”€ Churn prediction: % of casual users who actually churn
   â””â”€â”€ Alert: If accuracy drops below 80%
```

### Error Tracking & Alerting

```
Scenarios to Monitor:
â”œâ”€â”€ LLM Errors:
â”‚   â”œâ”€â”€ Timeout (> 10 seconds) â†’ Retry, fallback to previous segment
â”‚   â”œâ”€â”€ Rate limit (429) â†’ Queue for later
â”‚   â”œâ”€â”€ Invalid JSON â†’ Fallback to engagement score percentile
â”‚   â””â”€â”€ Low confidence (< 0.6) â†’ Flag for manual review
â”œâ”€â”€ Data Quality:
â”‚   â”œâ”€â”€ User with 0 app accesses classified as power â†’ Data error
â”‚   â”œâ”€â”€ Engagement score >1.0 or <0.0 â†’ Formula bug
â”‚   â””â”€â”€ 50% of users change segment in one week â†’ System instability
â”œâ”€â”€ Anomalies:
â”‚   â”œâ”€â”€ Org avg engagement drops >20% â†’ Investigate data pipeline
â”‚   â”œâ”€â”€ 100+ users suddenly go from power â†’ casual â†’ Potential outage
â”‚   â””â”€â”€ Churn rate spikes to 60% â†’ Something wrong with product
â””â”€â”€ Cost:
    â”œâ”€â”€ Daily LLM spend > $10 â†’ Investigate (expected $3-4/day)
    â””â”€â”€ BigQuery query cost > $20/day â†’ Review expensive queries

Alert Routing:
â”œâ”€â”€ Critical (PagerDuty): Data pipeline down, mass segment changes
â”œâ”€â”€ Warning (Slack): LLM latency >5sec, cost >budget, accuracy drop
â””â”€â”€ Info (Email): Weekly summary of segment distribution changes
```

### Prompt Version Control

```
Git Repository: "parable-user-segmentation-prompts"
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ user_segmentation_v1.2.txt (current production)
â”‚   â”‚   â””â”€â”€ Accuracy: 88%, Manager agreement: 87%
â”‚   â”œâ”€â”€ user_segmentation_v1.1.txt (previous)
â”‚   â”‚   â””â”€â”€ Accuracy: 85%, Manager agreement: 84%
â”‚   â””â”€â”€ user_segmentation_v1.3_beta.txt (testing)
â”‚       â””â”€â”€ Hypothesis: Adding churn prediction improves value
â”‚
â”œâ”€â”€ A/B Test:
â”‚   â”œâ”€â”€ 90% of users: v1.2 (production)
â”‚   â””â”€â”€ 10% of users: v1.3_beta (test)
â”‚
â””â”€â”€ Rollback: If issues, revert to v1.1 within minutes
```

---

## 7. Scale & Performance

### Petabyte-Scale Data Processing

```
Current State: 18 events (sample)
Production State: 50M SSO events/day Ã— 50 clients = 1B events/day (shared with Work Categorizer)

Data Volume for User Segmentation:
â”œâ”€â”€ Engagement scores: 10k users Ã— 200 bytes = 2 MB (refreshed nightly)
â”œâ”€â”€ User segments: 10k users Ã— 500 bytes = 5 MB (refreshed weekly for 20%)
â””â”€â”€ Negligible compared to SSO events (shared 36.5 TB/year)

Scaling Strategy:
â”œâ”€â”€ Reuse SSO data from Work Categorizer (no duplicate storage)
â”œâ”€â”€ Engagement score computation: Cheap SQL aggregations (no LLM)
â”œâ”€â”€ Segmentation: Only LLM for users with score changes (20%/week)
â””â”€â”€ Cost scales sub-linearly (only changed users, not all 10k every week)
```

### Latency Optimization

```
CEO Dashboard Requirements: < 2 seconds load time

Techniques:
â”œâ”€â”€ 1. Pre-computed Engagement Scores (nightly batch)
â”‚   â””â”€â”€ No real-time calculation, just lookup from table
â”œâ”€â”€ 2. Materialized View (refresh every 6 hours)
â”‚   â””â”€â”€ Segment distribution, ROI metrics pre-aggregated
â”œâ”€â”€ 3. Redis Caching (Memorystore)
â”‚   â””â”€â”€ Cache dashboard JSON for 1 hour
â””â”€â”€ 4. Lazy Loading
    â””â”€â”€ Load segment distribution first, recommendations on-demand

Latency Budget Breakdown (2 sec total):
â”œâ”€â”€ Frontend render: 400ms
â”œâ”€â”€ API call: 150ms
â”œâ”€â”€ BigQuery query (materialized view): 600ms
â”œâ”€â”€ Data serialization: 150ms
â””â”€â”€ Network: 200ms
```

### Cost Optimization

```
Monthly Costs for 50 Clients Ã— 10k Employees (500k users):

BigQuery:
â”œâ”€â”€ Storage: Shared with Work Categorizer (~$0 incremental)
â”œâ”€â”€ Queries: Engagement score calculation (nightly SQL) = $100/month
â””â”€â”€ Segmentation queries (weekly) = $50/month

LLM (User Segmentation Analyzer):
â”œâ”€â”€ 500k users Ã— 20% need re-segmentation/week = 100k users/week
â”œâ”€â”€ 100k users Ã— $0.012 per segmentation Ã— 4 weeks = $4,800/month
â””â”€â”€ Optimization: Only re-segment on score change >0.15 (not all users)
    â†’ Reduces to 50k users/week Ã— $0.012 Ã— 4 = $2,400/month

Cloud Run:
â”œâ”€â”€ Nightly engagement score batch: $50/month (15 min/day processing)
â”œâ”€â”€ Weekly segmentation batch: $100/month (1 hr/week processing)
â””â”€â”€ API endpoints: Shared with Work Categorizer ($0 incremental)

Total: ~$2.7k/month for 500k users
Per user: $0.0054/month (half a cent per user!)

Cost Breakdown:
â”œâ”€â”€ LLM costs: $2.4k/month (89% of total)
â”œâ”€â”€ BigQuery queries: $150/month (6%)
â””â”€â”€ Cloud Run: $150/month (6%)

Cost vs ROI:
â”œâ”€â”€ Monthly cost: $2.7k
â”œâ”€â”€ Annual cost: $32k
â”œâ”€â”€ Annual savings (1,000 employees): $1.1M
â””â”€â”€ ROI: 3,438% (for every $1 spent, save $34)
```

### Infrastructure (GCP Single-Tenant)

```
Per Client (Fortune 1000 company):
â”œâ”€â”€ Shared SSO data pipeline (reuse Work Categorizer infrastructure)
â”œâ”€â”€ Dedicated BigQuery tables:
â”‚   â”œâ”€â”€ user_engagement_scores (client-specific)
â”‚   â””â”€â”€ user_segments (client-specific)
â”œâ”€â”€ Shared Cloud Run jobs (parameterized by tenant_id)
â””â”€â”€ Isolated by tenant_id in all queries

Cost Efficiency:
â”œâ”€â”€ Reuse SSO ingestion (Pub/Sub, Cloud Run, BigQuery)
â”œâ”€â”€ Marginal cost for segmentation: $2.7k/month for 500k users
â””â”€â”€ Bundled with Work Categorizer: $3.3k (categorizer) + $2.7k (segmentation) = $6k total
```

### Failure Scenarios & Disaster Recovery

```
1. Engagement Score Computation Failure
   â”œâ”€â”€ Problem: Nightly batch job fails (SQL timeout, data corruption)
   â”œâ”€â”€ Mitigation:
   â”‚   â”œâ”€â”€ Retry 3 times with exponential backoff
   â”‚   â”œâ”€â”€ Fallback to yesterday's scores (1 day stale, acceptable)
   â”‚   â”œâ”€â”€ Alert Slack if batch fails >3 times
   â”‚   â””â”€â”€ Checkpointing: Process in batches of 1,000 users
   â””â”€â”€ Impact: Scores delayed 1 day (low severity)

2. LLM Segmentation Failure
   â”œâ”€â”€ Problem: Claude API down, cannot segment users
   â”œâ”€â”€ Mitigation:
   â”‚   â”œâ”€â”€ Use previous week's segments (7 days stale)
   â”‚   â”œâ”€â”€ Simple fallback: Percentile-based segmentation
   â”‚   â”‚   â””â”€â”€ If score >75th percentile â†’ Power
   â”‚   â”‚   â””â”€â”€ If score 25-75th â†’ Regular
   â”‚   â”‚   â””â”€â”€ If score <25th â†’ Casual
   â”‚   â”œâ”€â”€ Multi-provider: Fallback to OpenAI GPT-4
   â”‚   â””â”€â”€ Queue for retry when Claude recovers
   â””â”€â”€ Impact: Segments up to 7 days stale, but simple fallback functional

3. Segment Instability (Mass Changes)
   â”œâ”€â”€ Problem: 50% of users change segment in one week (data quality issue)
   â”œâ”€â”€ Mitigation:
   â”‚   â”œâ”€â”€ Anomaly detection: Alert if >30% change segments
   â”‚   â”œâ”€â”€ Rollback: Revert to previous week's segments
   â”‚   â”œâ”€â”€ Investigation: Review engagement score formula, data pipeline
   â”‚   â””â”€â”€ Manual review: IT team validates segments before publishing
   â””â”€â”€ Impact: Catch data quality issues before affecting users

4. Cost Overrun
   â”œâ”€â”€ Problem: LLM costs spike from $2.4k/month to $20k/month
   â”œâ”€â”€ Mitigation:
   â”‚   â”œâ”€â”€ Budget alert: Notify at 80% ($1.9k) and 100% ($2.4k)
   â”‚   â”œâ”€â”€ Auto-throttle: Pause segmentation if cost >150% of budget
   â”‚   â”œâ”€â”€ Investigation: Check if threshold (0.15 score change) too loose
   â”‚   â””â”€â”€ Emergency: Kill switch to halt all LLM calls
   â””â”€â”€ Impact: Financial, caught early before major overspend
```

---

## ğŸ“Š Whiteboard Diagram: User Segmentation Analyzer Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CEO / IT / HR DASHBOARD                          â”‚
â”‚  "User Segmentation: Power 30%, Regular 50%, Casual 20%             â”‚
â”‚   License Optimization: $200k/year in reclaimed licenses            â”‚
â”‚   Power User ROI: $900k/year from workflow automation"              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†‘
                              â”‚ Query (< 2 sec)
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              BIGQUERY (Materialized Views + Tables)                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚ user_        â”‚  â”‚ user_        â”‚  â”‚ sso_events_  â”‚              â”‚
â”‚  â”‚ segments     â”‚  â”‚ engagement_  â”‚  â”‚ raw          â”‚              â”‚
â”‚  â”‚ (10k users)  â”‚  â”‚ scores       â”‚  â”‚ (SHARED)     â”‚              â”‚
â”‚  â”‚              â”‚  â”‚ (10k users)  â”‚  â”‚              â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†‘                    â†‘                         â†‘
         â”‚                    â”‚                         â”‚
         â”‚ Write segments     â”‚ Write scores            â”‚ Ingest events
         â”‚ (weekly, 20%)      â”‚ (nightly, all)          â”‚ (real-time)
         â†“                    â†“                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚ CLOUD RUN JOB      â”‚  â”‚ CLOUD RUN JOB      â”‚         â”‚
â”‚ "user-segmentation â”‚  â”‚ "engagement-score- â”‚         â”‚
â”‚  -batch" (weekly)  â”‚  â”‚  calculator"       â”‚         â”‚
â”‚                    â”‚  â”‚  (nightly)         â”‚         â”‚
â”‚ For 2k users with  â”‚  â”‚                    â”‚         â”‚
â”‚ score changes:     â”‚â—„â”€â”¤ For 10k users:     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ 1. Fetch metrics   â”‚  â”‚ 1. SQL aggregation â”‚   (Shared SSO
â”‚ 2. Call User Seg   â”‚  â”‚    (count, avg,    â”‚    pipeline with
â”‚    Analyzer (LLM)  â”‚  â”‚     distinct apps) â”‚    Work Categorizer)
â”‚ 3. Store segments  â”‚  â”‚ 2. Calculate score â”‚
â”‚                    â”‚  â”‚ 3. Store in BQ     â”‚
â”‚ Cost: $24/week     â”‚  â”‚                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ Cost: $2/day       â”‚
         â†“              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ 2 LLM calls/user
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  USER SEGMENTATION ANALYZER AGENT          â”‚
â”‚                                            â”‚
â”‚  Tools:                                    â”‚
â”‚  â€¢ query_user_engagement_metrics           â”‚
â”‚  â€¢ calculate_engagement_score              â”‚
â”‚  â€¢ segment_user (LLM)                      â”‚
â”‚  â€¢ detect_engagement_anomalies             â”‚
â”‚  â€¢ recommend_interventions (LLM)           â”‚
â”‚                                            â”‚
â”‚  Output:                                   â”‚
â”‚  Segment (power/regular/casual) +          â”‚
â”‚  Confidence + Reasoning +                  â”‚
â”‚  Churn Risk + Personalized                 â”‚
â”‚  Recommendations                           â”‚
â”‚                                            â”‚
â”‚  Example:                                  â”‚
â”‚  "Power User (Multi-tool Specialist).      â”‚
â”‚   Confidence: 94%. Evidence: 450 accesses, â”‚
â”‚   12 apps, top 10% of org. Recommendations:â”‚
â”‚   Enable API access, reduce auth steps.    â”‚
â”‚   Savings: $9.75k/year."                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
         â”‚ All LLM calls traced
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TRACING & MONITORING                      â”‚
â”‚  â€¢ Cost: $2.7k/month (500k users)          â”‚
â”‚  â€¢ Latency: p95 < 3 sec                    â”‚
â”‚  â€¢ Accuracy: >85% (vs manager labels)      â”‚
â”‚  â€¢ Segment stability: >80%                 â”‚
â”‚  â€¢ Alerts: Slack/PagerDuty                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ—£ï¸ Common Interview Questions & Answers

### Q: "How is this different from Work Categorizer?"

**A:** "Great question - there's overlap but distinct use cases:

**Work Categorizer (Hypothesis 2.7):**

- **What:** Classifies users by job role (Engineer, Sales, PM, etc.)
- **Input:** App usage patterns (GitHub, Jira, Slack â†’ Engineer)
- **Output:** 'Software Engineer (Frontend)' with 95% confidence
- **Use case:** Role-based IT policies, training, tool recommendations
- **Complexity:** Higher (LLM analyzes app semantics, industry benchmarks)

**User Segmentation (Hypothesis 3.1):**

- **What:** Classifies users by engagement level (Power, Regular, Casual)
- **Input:** Engagement metrics (frequency, diversity, recency)
- **Output:** 'Power User (Multi-tool Specialist)' with 94% confidence
- **Use case:** License optimization, personalized UX, churn prevention
- **Complexity:** Lower (mostly SQL aggregations, simpler LLM task)

**Key Difference:**

- Work Categorizer asks: _WHAT_ does the user do? (job role)
- User Segmentation asks: _HOW MUCH_ does the user engage? (usage intensity)

**Can they work together?**
Yes! They're complementary:

```
User: Alice
â”œâ”€â”€ Work Categorizer â†’ 'Software Engineer (Frontend)'
â”‚   â””â”€â”€ Recommendations: Dev tool integrations, GitHub shortcuts
â””â”€â”€ User Segmentation â†’ 'Power User'
    â””â”€â”€ Recommendations: API access, advanced training

Combined recommendation:
'Alice is a Power User Engineer. Give her: API access for automation,
 GitHub/Jira bundling, and invite to Advanced Engineering Community.'
```

**System design similarity:**

- Both reuse same SSO data pipeline (no duplicate ingestion)
- Both use nightly/weekly batches (not real-time)
- Both have LLM + simple SQL hybrid architecture
- User Segmentation is ~40% cheaper ($2.7k vs $3.3k/month for 500k users)"

---

### Q: "Why LLM for segmentation when you could just use percentiles?"

**A:** "Excellent question - simple percentile bucketing (top 25% = power, bottom 25% = casual) is cheaper, but here's why agentic is better:

**Simple Percentile Approach:**

```python
if engagement_score > 0.75:  # Top 25%
    segment = "Power User"
elif engagement_score < 0.25:  # Bottom 25%
    segment = "Casual User"
else:
    segment = "Regular User"
```

**Pros:**

- $0 LLM cost
- Fast (instant)
- Deterministic

**Cons:**

- **Not explainable:** Why is user in top 25%? Just "high score"
- **No sub-segments:** Can't detect "Multi-tool Specialist" vs "Single-tool Expert"
- **No churn prediction:** Doesn't detect power â†’ casual transition
- **No personalized recommendations:** Can't say "Enable API access for this user"
- **Brittle:** If org average drops, percentiles shift (user can change segment without behavior change)

**Agentic Approach with LLM:**

```
LLM analyzes engagement metrics + trends + context
Output: "Power User (Multi-tool Specialist) BECAUSE:
         - 450 accesses (top 10%)
         - 12 unique apps (vs org avg 3.5)
         - Stable trend (no churn risk)
         Recommendations: API access, workflow automation"
```

**Pros:**

- **Explainable:** Manager/IT can see WHY user is power
- **Sub-segments:** Detects nuanced patterns (multi-tool vs single-tool power users)
- **Churn prediction:** "Was power, now casual â†’ at-risk"
- **Actionable recommendations:** "This power user needs API access to save 10 hrs/month"
- **Context-aware:** "User has high score but declining trend â†’ proactive intervention"

**Hybrid Approach (Best of Both Worlds):**

```
1. Use percentiles for initial bucketing (cheap, fast)
2. LLM for:
   - Users near boundaries (score 0.24 or 0.26 â†’ need nuanced judgment)
   - Anomalies (rapid score changes â†’ need churn analysis)
   - High-value users (power users â†’ worth $0.012 to generate custom recommendations)
3. Cache LLM results (don't re-segment if score unchanged)
```

**For Parable's use case:**

- LLM cost: $2.7k/month for 500k users (half a cent per user)
- Value: Personalized recommendations worth $1.1M/year
- ROI: 3,400% â†’ LLM cost is negligible compared to value
- Decision: Use LLM, especially for power users (high ROI) and at-risk users (churn prevention)"

---

### Q: "How do you prevent gaming the system?"

**A:** "Great question - users might try to game engagement scores to get perks. Here's our defense:

**Scenario 1: User tries to inflate engagement score**

```
User thinks: 'If I access 20 apps randomly, I'll become a power user
              and get API access + reduced auth steps.'
```

**Our defense:**

1. **Depth matters, not just breadth:**

   - Engagement score includes session duration
   - Random 5-second app opens â†’ low depth score
   - Formula: depth_score = AVG(session_duration) / org_avg_duration
   - Gaming attempt: User accesses 20 apps Ã— 5 sec = low depth (0.1/100 = 0.001)
   - Real power user: 5 apps Ã— 45 min = high depth (45/12 = 3.75)

2. **Consistency required:**

   - One-time spike doesn't change segment
   - Engagement score is 90-day rolling average
   - User needs sustained behavior over 3 months

3. **LLM detects anomalies:**

   ```
   LLM sees:
   - Days 1-60: 2 app accesses/week (casual pattern)
   - Days 61-90: 100 app accesses/week (sudden spike)

   LLM reasoning: "Anomalous behavior detected. User went from
                  casual to extreme high usage overnight. This
                  doesn't match organic power user patterns.
                  Likely gaming attempt. Confidence: 0.35 (low).
                  Recommend manual review."
   ```

4. **Manager validation:**
   - Segments shown to managers for review
   - Manager can override: "This is wrong, user is actually casual"
   - Overrides fed back as ground truth for future LLM prompts

**Scenario 2: User tries to appear casual to avoid work**

```
User thinks: 'If I stop using tools, I'll be labeled casual and
              they'll reclaim my license (fewer expectations).'
```

**Our defense:**

1. **Churn risk flags:**

   - LLM detects power â†’ casual transition: "User was power (score 0.85)
     for 60 days, suddenly dropped to casual (0.25). Churn risk: HIGH."
   - Alert manager: "Alice's engagement dropped 70% in 30 days. Check in?"

2. **Manager visibility:**

   - Managers see team engagement trends
   - Sudden drop triggers review: "Why is Alice disengaging?"

3. **Work output correlation:**
   - User Segmentation is INPUT, not OUTPUT
   - Manager still evaluates work quality (project completion, sprint velocity)
   - Low tool engagement BUT high work output â†’ user found workarounds (investigate)

**Gaming is hard because:**

- Engagement score has 5 dimensions (frequency, diversity, recency, consistency, depth)
- Gaming requires sustained 90-day effort across all dimensions
- LLM detects anomalies in patterns
- Managers review segments (human-in-the-loop)
- Segments are descriptive (what IS), not prescriptive (what SHOULD BE)
  â†’ Low engagement doesn't mean bad employee, just different tool usage"

---

### Q: "How do you handle seasonal patterns (holidays, year-end)?"

**A:** "Excellent question - engagement naturally fluctuates with seasons:

**Problem:**

```
December (year-end holidays):
â”œâ”€â”€ Avg engagement score drops from 0.45 â†’ 0.30 (33% drop)
â”œâ”€â”€ 40% of users appear to churn (no activity for 2+ weeks)
â””â”€â”€ System incorrectly flags hundreds as "casual" or "at-risk"

Result: False alarms, wasted IT time investigating normal holiday patterns
```

**Solution 1: Seasonal Baseline Adjustment**

```python
# Instead of: engagement_score = absolute_metrics
# Use: engagement_score = metrics_vs_seasonal_baseline

seasonal_baseline = AVG(engagement_score) for same week last year
user_score_adjusted = user_score / seasonal_baseline

Example:
- User score in Dec Week 3: 0.30
- Org avg in Dec Week 3 this year: 0.30
- Org avg in Dec Week 3 last year: 0.28
- Seasonal baseline: 0.28
- Adjusted score: 0.30 / 0.30 Ã— 0.45 (annual avg) = 0.45
â†’ User's RELATIVE engagement unchanged, not flagged as casual
```

**Solution 2: LLM Context Awareness**

```
Prompt includes seasonal context:

"Current date: December 23, 2025 (holiday week).
 Organization-wide engagement is 35% below annual average.
 When classifying this user, account for expected seasonal drop.
 Do not flag users as 'at-risk churn' if decline matches org-wide pattern."

LLM reasoning:
"User engagement dropped 30%, but org-wide drop is 35%.
 User is actually OUTPERFORMING during holiday season.
 Segment: Regular User (stable). Churn risk: Low."
```

**Solution 3: Trend Analysis Over Multiple Seasons**

```
Don't compare Dec 2025 to Nov 2025 (sequential)
Compare Dec 2025 to Dec 2024 (year-over-year)

If user_score_dec_2025 < user_score_dec_2024:
    â†’ True decline (not seasonal)
Else:
    â†’ Seasonal pattern (ignore)
```

**Solution 4: Segment Stability (Don't Change Too Fast)**

```
Require 2 consecutive anomalous periods before changing segment:

Week 1: User score drops to 0.25 â†’ Flag as "watch"
Week 2: User score still 0.25 â†’ Flag as "at-risk"
Week 3: User score still 0.25 â†’ Change segment to "Casual"

Holiday scenario:
Dec Week 1: Score 0.30 (holiday) â†’ Flag "watch"
Dec Week 2: Score 0.32 (holiday) â†’ Still "watch"
Jan Week 1: Score 0.45 (back to normal) â†’ Clear "watch" flag, no segment change
```

**Monitoring:**

```
Dashboard: "Seasonal Engagement Trends"
â”œâ”€â”€ Current week vs same week last year
â”œâ”€â”€ Expected drop: 30-40% in Dec, 15-20% in Jul (summer)
â”œâ”€â”€ Alert if drop > 2Ã— expected (e.g., 60% drop in Dec â†’ investigate)
â””â”€â”€ Suppress churn alerts during known holiday periods
```

**Result:**

- Fewer false positives (holiday != churn)
- Accurate year-over-year comparisons
- LLM-aware of seasonal context
- Stable segments (users don't flip-flop during holidays)"

---

## âœ… Interview Readiness Checklist

**System Design Fundamentals:**

- [ ] I can draw the User Segmentation Analyzer architecture on whiteboard
- [ ] I can explain all 7 key areas (business, agent, pipeline, RBAC, evaluation, tracing, scale)
- [ ] I can calculate costs for 500k users ($2.7k/month, $0.0054/user)
- [ ] I can discuss latency optimization (<2 sec dashboard, nightly batch for scores)

**Agentic Workflows:**

- [ ] I can explain why agentic > simple percentiles (explainability, sub-segments, churn prediction)
- [ ] I can design the agent with tools, LLM prompting, engagement score formula
- [ ] I can describe natural language outputs for users AND IT
- [ ] I can discuss evaluation (manager validation, cohort analysis, A/B test)

**Comparison to Other Hypotheses:**

- [ ] I can contrast User Segmentation vs Work Categorizer (engagement vs role)
- [ ] I can explain how they complement each other (combine for better recommendations)
- [ ] I can discuss why segmentation is simpler/cheaper than categorization

**Production Engineering:**

- [ ] I can describe BigQuery architecture (reuse SSO data, engagement scores nightly)
- [ ] I can explain single-tenant isolation per Fortune 1000 client
- [ ] I can design RBAC matrix (CEO sees aggregates, managers see teams, employees see own)
- [ ] I can describe LLM tracing (cost, latency, tokens, prompt version)

**Use Cases:**

- [ ] I can discuss license optimization ($200k/year reclaimed from casual users)
- [ ] I can explain personalized interventions (power users get API access, casual get simplified UX)
- [ ] I can describe churn prevention (detect power â†’ casual transition, intervene)

**Scale & Performance:**

- [ ] I can discuss data volume (reuse 36.5 TB SSO data, only 7 MB incremental)
- [ ] I can optimize costs ($2.7k/month for 500k users, only segment 20% with changes each week)
- [ ] I can calculate ROI ($32k/year cost vs $1.1M/year savings = 3,400% ROI)
- [ ] I can handle edge cases (gaming, seasonal patterns, rapid changes)

**Interview Flow:**

- [ ] I can smoothly transition from data analysis to system design
- [ ] I can handle common questions (vs Work Categorizer, why LLM, gaming, seasonal)
- [ ] I can reference the 5 CEO questions throughout
- [ ] I can quantify ROI for every design decision ($1.1M/year savings)

**Sample Data Evidence:**

- [ ] I know the CORRECT segmentation: 50% power, 10% regular, 40% casual (NOT 30/40/30)
- [ ] I can acknowledge sample limitations (13-second window, can't distinguish true casual vs "not yet accessed")
- [ ] I can extrapolate to production patterns (20-30% power, 40-50% regular, 20-30% casual)

---

**You're now ready to discuss the User Segmentation Analyzer system design in your Parable interview!**
