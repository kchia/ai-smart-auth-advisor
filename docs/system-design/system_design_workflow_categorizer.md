# System Design & Production Architecture

**Purpose:** Prepare for the system design portion of the Parable interview
**Focus:** Designing production-grade agentic workflows at petabyte scale for Fortune 1000 clients

**â­ CRITICAL:** This interview is testing your ability to design the ACTUAL Parable product:

- Work categorizer agentic workflow (first 3-month deliverable)
- Organizational observability platform
- CEO-facing insights at Fortune 1000 scale
- Production concerns: RBAC, privacy, tracing, evaluation

---

## ðŸŽ¯ Interview Flow: Data Analysis â†’ System Design

### Typical Transition (60-minute interview)

**Minutes 1-30:** Collaborative data analysis

- You've just finished coding together, tested a hypothesis
- Found interesting patterns (e.g., auth friction, user segmentation)
- Quantified time waste with ROI calculations

**Minutes 30-60:** System design discussion

- **Interviewer:** "Interesting findings! Now, how would you build a production system that delivers these insights to CEOs at scale?"
- **You:** _This is where Phase 3 preparation kicks in_

---

## ðŸ—ï¸ System Design Framework (Parable-Specific)

### The 7 Key Areas to Cover

When designing ANY agentic workflow solution for Parable, address these 7 areas:

1. **Business Requirements** (CEO-level)

   - Which of the 5 CEO questions does this answer?
   - What is the quantified ROI? (time saved, $ saved)
   - What decisions can CEOs make with this insight?

2. **Agentic Workflow Architecture**

   - Agent design (tools, LLM prompting, flow)
   - Why agentic > traditional ML?
   - Natural language outputs for CEO audience

3. **Data Pipeline**

   - Ingestion (SSO logs from 50+ Fortune 1000 clients)
   - Storage (BigQuery, Iceberg data lake)
   - Processing (Cloud Run jobs, Pub/Sub)

4. **RBAC & Privacy**

   - Who can see what data?
   - Row-level security in BigQuery
   - PII handling, anonymization

5. **Evaluation & Quality**

   - Ground truth labels (how to validate agent outputs)
   - A/B testing framework
   - Success metrics

6. **Tracing & Monitoring**

   - LLM call logging (cost, latency, tokens)
   - Error tracking, alerting
   - Cost management across 500k users

7. **Scale & Performance**
   - Petabyte-scale data processing
   - Latency requirements (< 3 sec for dashboards)
   - Cost optimization ($3k/month LLM costs for 10k employees)

---

## ðŸŽ¯ Example: Work Categorizer Agentic Workflow (First 3-Month Deliverable)

### 1. Business Requirements

**Problem Statement:**
CEOs need to understand how employees spend their time, but they lack visibility into work patterns across thousands of employees.

**CEO Questions Answered:**

- "How can we use AI to make teams 100x productive?" (personalized interventions)
- "Where can we automate?" (role-based optimizations)

**ROI:**

- Without categorization: Generic IT policies â†’ 5-10% productivity loss (conservative estimate)
- 1,000 employees Ã— $50/hr Ã— 40 hrs/week Ã— 5% = **$1M/week = $50M/year waste**
- With categorization: Role-specific optimizations â†’ 10-15% productivity gain
- **Note:** These are assumptions to be validated through A/B testing in production

**CEO-Level Decisions Enabled:**

- "Our engineering team wastes 4.2 hrs/week on tool friction â†’ invest in dev tools integration"
- "Sales team underutilizes CRM â†’ provide training or consolidate tools"

---

### 2. Agentic Workflow Architecture

**Why Agentic > Traditional ML:**

| Aspect             | Traditional ML (K-means)         | Agentic Workflow                                                                                              |
| ------------------ | -------------------------------- | ------------------------------------------------------------------------------------------------------------- |
| **Output**         | "User in cluster 3"              | "Software Engineer (Frontend) BECAUSE..."                                                                     |
| **Explainability** | Black box                        | Natural language with citations                                                                               |
| **Adaptability**   | Requires retraining for new apps | Few-shot learning, adapts immediately                                                                         |
| **CEO-friendly**   | No                               | Yes - actionable insights                                                                                     |
| **Example**        | "Cluster 3 has 450 employees"    | "Engineers (450) waste 4.2 hrs/week on context switching. ROI: $2.1M/year if we implement workflow bundling." |

**Agent Design:**

```
Work Categorizer Agent
â””â”€â”€ Input: user_id
â””â”€â”€ Tools:
    â”œâ”€â”€ query_user_app_history(user_id, days=90)
    â”‚   â””â”€â”€ Returns: {GitHub: 250 accesses, Slack: 180, Jira: 90, Figma: 30}
    â”œâ”€â”€ query_app_cooccurrence(apps_list)
    â”‚   â””â”€â”€ Returns: Apps commonly used together in sessions
    â”œâ”€â”€ get_industry_benchmarks(industry)
    â”‚   â””â”€â”€ Returns: Typical app patterns for known roles
    â”œâ”€â”€ categorize_work_pattern(app_signature) [LLM]
    â”‚   â””â”€â”€ Input: App usage metrics
    â”‚   â””â”€â”€ Output: Work category + confidence + reasoning
    â””â”€â”€ suggest_optimizations(category, current_setup) [LLM]
        â””â”€â”€ Input: Category + current IT policies
        â””â”€â”€ Output: Personalized productivity recommendations

â””â”€â”€ Flow:
    1. Query BigQuery for user's app access patterns (last 90 days)
    2. Extract app signature: frequency, recency, co-occurrence
    3. LLM analyzes with few-shot prompting:
       Prompt: "Given these app access patterns, what type of work
               does this user do? Evidence: [app metrics]. Known
               patterns: [industry benchmarks]. Categorize with
               reasoning and confidence."
    4. LLM responds with structured output:
       {
         "category": "Software Engineer (Frontend/Full-stack)",
         "confidence": 0.95,
         "reasoning": "Heavy GitHub usage (250) + Jira (90) +
                      Figma (30) indicates frontend engineering",
         "evidence": ["GitHub access 88% of activity",
                     "Figma suggests UI work"]
       }
    5. Generate optimization recommendations (another LLM call):
       "For engineers: reduce auth steps for GitHub/Jira,
        bundle dev tools, alert on excessive Slack switching.
        Potential savings: 3-5 hrs/week = $7.5k-12.5k/year per eng."
    6. Store results in BigQuery for CEO dashboard

â””â”€â”€ Output to CEO Dashboard:
    "Work Category Distribution:
     - Engineering (45%) - 450 employees
       â†’ Opportunity: Reduce tool friction â†’ $2.1M/year savings
     - Sales (20%) - 200 employees
       â†’ Opportunity: CRM consolidation â†’ $800k/year savings
     - Customer Support (15%) - 150 employees
     - Operations (10%) - 100 employees
     - Management (10%) - 100 employees

     Top Recommendation:
     Implement role-based IT policies to reduce context switching
     and auth friction for engineering team. Projected ROI: $2.1M/year."
```

**Actual LLM Prompt (Memorize for Interview):**

```
System: You are a work categorization expert for enterprise productivity analysis.

User: Analyze the following employee's app usage patterns from the last 90 days and categorize their primary work role.

App Access Data:
- GitHub: 250 accesses (code repository, version control)
- Slack: 180 accesses (team communication)
- Jira: 90 accesses (project management, issue tracking)
- Figma: 30 accesses (design and prototyping tool)

Industry Benchmarks (few-shot examples):
- Software Engineers typically use: GitHub (high), Jira (medium), Slack (medium), IDEs
- Designers typically use: Figma (high), Adobe Suite (high), Slack (medium)
- Sales typically use: Salesforce (high), Gmail (high), Calendar (high), HubSpot
- Product Managers typically use: Jira (high), Figma (medium), Slack (high), Notion

Task:
1. Categorize this employee's primary work role
2. Provide confidence score (0.0 to 1.0)
3. Explain reasoning with specific citations to the data
4. List supporting evidence

Output format (JSON):
{
  "category": "Software Engineer (Frontend/Full-stack)",
  "confidence": 0.95,
  "reasoning": "Heavy GitHub usage (250 accesses) indicates active coding and version control work. Jira access (90 times) suggests agile development workflow with sprint planning. Figma usage (30 accesses) indicates involvement in UI/UX work, suggesting frontend or full-stack engineering rather than pure backend. This pattern strongly matches typical frontend engineer signature.",
  "evidence": [
    "GitHub accounts for 44% of total app usage (250/570 total accesses)",
    "GitHub + Jira + Slack = 88% of activity, matching core engineering workflow",
    "Figma presence (5% of usage) differentiates from backend engineers",
    "No backend-specific tools (database clients, cloud consoles) detected"
  ],
  "sub_category": "Frontend-leaning full-stack engineer"
}
```

**Why this prompt works:**

- Few-shot examples guide the LLM (shows patterns for different roles)
- Structured output format ensures consistent, parseable responses
- Requires explicit confidence scoring for downstream filtering
- Asks for evidence/citations (explainability for CEO dashboards)
- Includes sub-categorization for nuanced roles

---

### 3. Data Pipeline (GCP Architecture)

**Ingestion:**

```
SSO Providers (Okta, Google Workspace, Azure AD)
    â†“ Webhooks / Polling
Cloud Pub/Sub Topic: "sso-events-raw"
    â†“ Subscribe
Cloud Run Job: "sso-event-ingestion"
    â†“ Parse, validate, enrich
BigQuery Table: "sso_events_raw"
    â”œâ”€â”€ Partitioned by: date (YYYYMMDD)
    â”œâ”€â”€ Clustered by: actor_id, target_id
    â””â”€â”€ Row-level security: tenant_id (single-tenant isolation)
```

**Processing (Nightly Batch):**

```
Cloud Scheduler (cron: 0 2 * * *)  # Run at 2am daily
    â†“ Trigger
Cloud Run Job: "work-categorizer-batch"
    â”œâ”€â”€ Query BigQuery for users needing recategorization:
    â”‚   â”œâ”€â”€ 90 days since last categorization (scheduled refresh)
    â”‚   â”œâ”€â”€ OR manager override flag (immediate recategorization)
    â”‚   â”œâ”€â”€ OR pattern change score > threshold (cheap heuristic:
    â”‚   â”‚   count of new apps accessed > 3, or 50% change in app mix)
    â”‚   â””â”€â”€ Pre-compute pattern change scores daily (simple SQL, no LLM)
    â”œâ”€â”€ For each user (5,556 users/day across 50 clients):
    â”‚   â”œâ”€â”€ Extract app usage metrics (last 90 days)
    â”‚   â”œâ”€â”€ Call Work Categorizer Agent (2 LLM calls)
    â”‚   â”œâ”€â”€ Cost: ~$0.02 per categorization
    â”‚   â””â”€â”€ Store result in BigQuery
    â””â”€â”€ Refresh materialized views for CEO dashboard
        â””â”€â”€ Total runtime: ~2 hours for 5,556 users
```

**Storage:**

```
BigQuery Dataset: "organizational_insights"
â”œâ”€â”€ Table: "sso_events_raw"
â”‚   â””â”€â”€ 1 billion events/day Ã— 365 days = 365B events
â”‚   â””â”€â”€ ~100 bytes/event = 36.5 TB/year
â”‚   â””â”€â”€ Cost: ~$750/month storage + $50k-500k/month queries
â”‚
â”œâ”€â”€ Table: "work_categories"
â”‚   â”œâ”€â”€ Schema: user_id, category, confidence, reasoning, last_updated
â”‚   â”œâ”€â”€ Rows: 500k users (50 clients Ã— 10k avg employees)
â”‚   â””â”€â”€ Cost: Negligible (~500 MB)
â”‚
â””â”€â”€ Materialized View: "productivity_metrics_dashboard"
    â”œâ”€â”€ Refreshed every 6 hours
    â”œâ”€â”€ Pre-aggregates for CEO dashboard (< 3 sec load time)
    â””â”€â”€ Partitioned by: client_id, department
```

**Iceberg Data Lake (Long-term Archive):**

```
Cloud Storage Bucket: "parable-data-lake"
â”œâ”€â”€ Format: Apache Iceberg (ACID transactions, schema evolution)
â”œâ”€â”€ Data: SSO events older than 90 days
â”œâ”€â”€ Cost: ~$0.02/GB/month (~$20/TB/year) vs BigQuery ($20/TB/month)
â”‚   â””â”€â”€ 12x cheaper for archival! $750/year vs $9,000/year for 365 TB
â””â”€â”€ Use case: Historical analysis, compliance, audit trails, 7-year retention
```

---

### 4. RBAC & Privacy

**Access Control Matrix:**

| Role                 | Data Access                                             | Example Query Restriction                                                                     |
| -------------------- | ------------------------------------------------------- | --------------------------------------------------------------------------------------------- |
| **CEO**              | Org-wide aggregates ONLY (no names, no individual data) | `SELECT category, COUNT(*) FROM work_categories WHERE tenant_id = X GROUP BY category`        |
| **Manager**          | Direct reports ONLY (identifiable)                      | `WHERE actor_id IN (SELECT employee_id FROM org_hierarchy WHERE manager_id = SESSION_USER())` |
| **IT/Security**      | Full access with audit trail                            | All data, but every query logged to `access_audit_log`                                        |
| **Employee**         | Own data ONLY                                           | `WHERE actor_id = SESSION_USER()`                                                             |
| **External Auditor** | Anonymized aggregates ONLY                              | No PII, only statistical summaries                                                            |

**BigQuery Row-Level Security (RLS):**

```sql
-- CEO policy: Only aggregates, no individual access
CREATE ROW ACCESS POLICY ceo_aggregate_only
ON organizational_insights.work_categories
GRANT TO ('group:ceos@company.com')
FILTER USING (FALSE);  -- CEOs can't query this table directly
-- Instead, they query pre-aggregated materialized views

-- Manager policy: Direct reports only
CREATE ROW ACCESS POLICY manager_team_filter
ON organizational_insights.sso_events_raw
GRANT TO ('group:managers@company.com')
FILTER USING (
    actor_id IN (
        SELECT employee_id
        FROM organizational_insights.org_hierarchy
        WHERE manager_id = SESSION_USER()
    )
);

-- IT/Security policy: Full access with audit logging
CREATE ROW ACCESS POLICY it_full_access_audited
ON organizational_insights.sso_events_raw
GRANT TO ('group:it-security@company.com')
FILTER USING (TRUE);  -- Full access
-- Note: All queries from this group logged via BigQuery audit logs
```

**PII Handling:**

```
Sensitive Fields in SSO Logs:
â”œâ”€â”€ actor.id (employee ID) â†’ Hash with SHA-256 + salt per tenant
â”œâ”€â”€ actor.displayName â†’ Redact entirely for CEO dashboards
â”œâ”€â”€ client.ipAddress â†’ Anonymize last octet (e.g., 192.168.1.XXX)
â”œâ”€â”€ client.geographicalContext.city â†’ Keep (not PII)
â””â”€â”€ target[].id (app ID) â†’ Keep (not PII)

Data Retention:
â”œâ”€â”€ Raw SSO events: 90 days in BigQuery (hot storage)
â”œâ”€â”€ Aggregated metrics: 7 years in Iceberg (compliance)
â””â”€â”€ PII purge: GDPR right-to-be-forgotten requests honored within 30 days
```

**Privacy-Preserving Analytics:**

```
Differential Privacy for CEO Dashboards:
- Add Laplace noise to small counts (< 10 users) to prevent re-identification
- Example: "Engineering: 7 employees" â†’ "Engineering: 5-10 employees"
- Use BigQuery's differential privacy SQL functions

k-Anonymity:
- Ensure every reported group has â‰¥ k users (k=10 recommended)
- If "ML Engineer" has only 3 users â†’ group into "Engineering (Other)"
```

---

### 5. Evaluation & Quality

**Ground Truth Labels:**

```
HR System Integration:
â”œâ”€â”€ Extract: employee_id, job_title, department, hire_date
â”œâ”€â”€ Map job titles to categories:
â”‚   "Software Engineer" â†’ Engineering
â”‚   "Account Executive" â†’ Sales
â”‚   "Customer Success Manager" â†’ Customer Support
â””â”€â”€ Create labeled dataset: 500 employees across diverse roles

Evaluation Pipeline:
â”œâ”€â”€ Run work categorizer on labeled users
â”œâ”€â”€ Compare agent category vs HR job title
â”œâ”€â”€ Accuracy = % of correct matches (Target: >80%)
â”œâ”€â”€ Confusion matrix: Which categories are confused?
â””â”€â”€ Iterate on prompt engineering to improve accuracy
```

**Explainability Testing:**

```
Manager Validation:
â”œâ”€â”€ Show 50 managers their team's categorizations + reasoning
â”œâ”€â”€ Survey: "Does this match reality?" (Yes/No + confidence 1-5)
â”œâ”€â”€ Target: >85% "Yes" responses, >4.0/5 avg confidence
â””â”€â”€ Use feedback to refine LLM prompts
```

**Multi-Role Categorization:**

```
Challenge: Many employees have multiple roles (e.g., "Engineering Manager")

Solution: Allow multiple categories with confidence scores
â”œâ”€â”€ LLM outputs array of categories:
â”‚   [
â”‚     {"role": "Engineering", "confidence": 0.7, "time_allocation": "60%"},
â”‚     {"role": "Management", "confidence": 0.6, "time_allocation": "40%"}
â”‚   ]
â”œâ”€â”€ Primary category = highest confidence
â”œâ”€â”€ Secondary categories shown in dashboard with percentages
â””â”€â”€ Enables hybrid role-based optimizations:
    "This Engineering Manager needs both dev tools AND leadership training"

Evaluation:
â”œâ”€â”€ Compare to HR titles: "Engineering Manager" â†’ should detect BOTH categories
â”œâ”€â”€ Target: >75% accuracy for hybrid roles (harder than single-role)
â””â”€â”€ Edge case: IC promoted to manager mid-quarter â†’ watch for category drift
```

**Adaptability Testing (Key advantage of agentic over ML):**

```
New App Introduction:
â”œâ”€â”€ Scenario: Company adopts "Linear" (project management tool)
â”œâ”€â”€ Traditional ML: Requires retraining entire model (weeks)
â”œâ”€â”€ Agentic workflow: Adapts immediately via few-shot learning
â”‚   â””â”€â”€ Prompt includes: "Linear is a project management tool
â”‚                         similar to Jira. Update your reasoning."
â”œâ”€â”€ Test: Does agent correctly incorporate Linear into categorization?
â””â”€â”€ Success: Agent categorizes users with Linear as "Engineering" or "Product"
```

**A/B Testing Framework:**

```
Experiment: Work Categorizer Impact on Productivity
â”œâ”€â”€ Control Group (1,000 employees):
â”‚   â””â”€â”€ Generic IT policies (no role-based optimizations)
â”œâ”€â”€ Treatment Group (1,000 employees):
â”‚   â””â”€â”€ Role-specific optimizations based on work categorization
â”‚       (e.g., reduced auth for engineers, bundled tools)
â”œâ”€â”€ Metrics:
â”‚   â”œâ”€â”€ Self-reported productivity (weekly survey: 1-5 scale)
â”‚   â”œâ”€â”€ Time savings (measured via auth time, context switches)
â”‚   â”œâ”€â”€ Task completion rate (sprint velocity for eng teams)
â”‚   â””â”€â”€ User satisfaction with IT tools (quarterly NPS)
â”œâ”€â”€ Duration: 3 months
â””â”€â”€ Success: Treatment group shows 10-15% productivity improvement
```

**Success Metrics:**
| Metric | Target | Measurement |
|--------|--------|-------------|
| **Accuracy** | >80% correct categorization | Compare to HR job titles |
| **Explainability** | >4.0/5 stakeholder confidence | Manager surveys |
| **Adoption** | >70% of recommendations implemented | IT team tracking |
| **Impact** | $1M+ quantified time savings in 6 months | A/B test results |
| **Latency** | <3 sec dashboard load time | p95 latency monitoring |
| **Cost** | <$5k/month LLM costs for 10k employees | BigQuery + LLM billing |

---

### 6. Tracing & Monitoring

**LLM Call Tracing (CRITICAL for agentic workflows):**

```json
Every LLM call logged to BigQuery table: "llm_call_traces"
{
  "trace_id": "abc123-def456",
  "timestamp": "2025-01-15T10:30:00Z",
  "agent": "work_categorizer",
  "user_id": "hash_abc123",
  "tenant_id": "fortune1000_client_42",

  "llm_calls": [
    {
      "call_id": "llm_1",
      "model": "claude-sonnet-4.5",
      "purpose": "categorize_work_pattern",
      "prompt_tokens": 850,
      "completion_tokens": 420,
      "cost_usd": 0.0128,
      "latency_ms": 1850,
      "prompt_version": "v2.3",
      "temperature": 0.2,
      "max_tokens": 500
    },
    {
      "call_id": "llm_2",
      "model": "claude-sonnet-4.5",
      "purpose": "suggest_optimizations",
      "prompt_tokens": 650,
      "completion_tokens": 380,
      "cost_usd": 0.0104,
      "latency_ms": 1620,
      "prompt_version": "v1.8",
      "temperature": 0.3,
      "max_tokens": 400
    }
  ],

  "total_cost_usd": 0.0232,
  "total_latency_ms": 3470,
  "category_output": "Software Engineer (Frontend)",
  "confidence": 0.95,
  "error": null
}
```

**Monitoring Dashboards (Grafana + BigQuery):**

```
1. LLM Cost Dashboard
   â”œâ”€â”€ Total LLM spend: $X/day, $Y/month
   â”œâ”€â”€ Cost per client (top 10 by spend)
   â”œâ”€â”€ Cost per agent type (work categorizer, auth advisor, etc.)
   â””â”€â”€ Alert: If daily cost > $500 (10% over budget)

2. LLM Performance Dashboard
   â”œâ”€â”€ p50, p95, p99 latency by agent type
   â”œâ”€â”€ Error rate (timeouts, rate limits, invalid responses)
   â”œâ”€â”€ Token usage trends (prompt vs completion)
   â””â”€â”€ Alert: If p95 latency > 5 seconds

3. Agent Quality Dashboard
   â”œâ”€â”€ Categorization accuracy (vs ground truth)
   â”œâ”€â”€ Confidence score distribution
   â”œâ”€â”€ User feedback ratings (helpfulness 1-5)
   â””â”€â”€ Alert: If accuracy drops below 75%

4. Data Pipeline Dashboard
   â”œâ”€â”€ BigQuery query costs (top 10 expensive queries)
   â”œâ”€â”€ Data freshness (time since last refresh)
   â”œâ”€â”€ Ingestion lag (SSO event â†’ BigQuery)
   â””â”€â”€ Alert: If ingestion lag > 5 minutes
```

**Error Tracking & Alerting:**

```
Scenarios to Monitor:
â”œâ”€â”€ LLM timeout (> 10 seconds)
â”œâ”€â”€ LLM rate limit exceeded (429 errors)
â”œâ”€â”€ Invalid LLM response (unparseable JSON)
â”œâ”€â”€ Low confidence categorization (< 0.6)
â”œâ”€â”€ BigQuery quota exceeded
â”œâ”€â”€ Data pipeline failure (Cloud Run job fails)
â””â”€â”€ Dashboard load time > 5 seconds

Alert Routing:
â”œâ”€â”€ Critical (PagerDuty): Data pipeline down, BigQuery quota exceeded
â”œâ”€â”€ Warning (Slack): LLM latency p95 > 5 sec, cost > budget
â””â”€â”€ Info (Email): Weekly summary of agent performance
```

**Prompt Version Control:**

```
Git Repository: "parable-prompts"
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ work_categorizer_v2.3.txt (current production)
â”‚   â”œâ”€â”€ work_categorizer_v2.2.txt (previous)
â”‚   â””â”€â”€ work_categorizer_v2.4_beta.txt (A/B testing)
â”œâ”€â”€ Track which prompt version generated which categorization
â”œâ”€â”€ A/B test new prompts before rolling out to all users
â””â”€â”€ Rollback if new prompt version degrades quality
```

---

### 7. Scale & Performance

**Petabyte-Scale Data Processing:**

```
Current State: 18 events (sample)
Production State: 1 billion events/day across 50 clients

Scaling Strategy:
â”œâ”€â”€ Partition BigQuery tables by date + client (tenant_id)
â”œâ”€â”€ Cluster by actor_id, target_id for query performance
â”œâ”€â”€ Materialize common aggregations to avoid full scans
â””â”€â”€ Archive events > 90 days to Iceberg (cheaper storage)

Example Query Optimization:
-- BAD: Full table scan (scans 365 TB!)
SELECT actor_id, COUNT(*)
FROM sso_events_raw
WHERE published > '2024-01-01';

-- GOOD: Partition pruning + clustering (scans 100 GB)
SELECT actor_id, COUNT(*)
FROM sso_events_raw
WHERE published BETWEEN '2025-01-01' AND '2025-01-31'  -- Partition pruning
  AND tenant_id = 'fortune1000_client_42'              -- Cluster pruning
GROUP BY actor_id;
```

**Latency Optimization:**

```
CEO Dashboard Requirements: < 3 seconds load time

Techniques:
â”œâ”€â”€ 1. Materialized Views (refresh every 6 hours)
â”‚   â””â”€â”€ Pre-aggregate work category distributions, productivity metrics
â”œâ”€â”€ 2. BigQuery BI Engine (in-memory cache)
â”‚   â””â”€â”€ Cache 100 GB of hot data for sub-second queries
â”œâ”€â”€ 3. Frontend Caching (Memorystore Redis)
â”‚   â””â”€â”€ Cache dashboard JSON for 1 hour
â””â”€â”€ 4. Lazy Loading (only fetch visible charts initially)

Latency Budget Breakdown (3 sec total):
â”œâ”€â”€ Frontend render: 500ms
â”œâ”€â”€ API call: 200ms
â”œâ”€â”€ BigQuery query (cached materialized view): 800ms
â”œâ”€â”€ Data serialization: 200ms
â””â”€â”€ Network: 300ms
```

**Cost Optimization:**

```
Monthly Costs for 50 Clients Ã— 10k Employees (500k users):

BigQuery:
â”œâ”€â”€ Storage: 365 TB @ $20/TB/month = $7,300/month
â”œâ”€â”€ Queries: ~$50k-100k/month (depends on usage)
â””â”€â”€ Optimization: Materialize views, partition pruning

LLM (Work Categorizer):
â”œâ”€â”€ 500k users Ã· 90 days = 5,556 categorizations/day
â”œâ”€â”€ $0.02 per categorization Ã— 5,556 = $111/day
â””â”€â”€ Monthly: $3,330/month (very cheap!)

Cloud Run:
â”œâ”€â”€ Batch jobs: $500/month (2 hrs/day processing)
â””â”€â”€ API endpoints: $2k/month (auto-scaling)

Pub/Sub:
â”œâ”€â”€ 1 billion messages/day @ $40/million = $40/day
â””â”€â”€ Monthly: $1,200/month

Total: ~$64k-114k/month for 500k users
Per user: $0.13-0.23/month (very reasonable!)
```

**Infrastructure (GCP Single-Tenant Architecture):**

```
Per Client (Fortune 1000 company):
â”œâ”€â”€ Isolated GCP Project
â”‚   â””â”€â”€ VPC, KMS keys, service accounts (no shared resources)
â”œâ”€â”€ Dedicated BigQuery dataset
â”‚   â””â”€â”€ Row-level security enforced per tenant_id
â”œâ”€â”€ Dedicated Cloud Run services
â”‚   â””â”€â”€ Environment variables configure client-specific settings
â””â”€â”€ Shared Code Pipeline
    â””â”€â”€ Same Docker image deployed across all client projects
    â””â”€â”€ Parameterized by tenant_id
```

**Failure Scenarios & Disaster Recovery:**

```
1. BigQuery Outage
   â”œâ”€â”€ Problem: BigQuery region goes down, CEO dashboard unavailable
   â”œâ”€â”€ Mitigation:
   â”‚   â”œâ”€â”€ Multi-region BigQuery dataset replication (us-central1 + us-east1)
   â”‚   â”œâ”€â”€ Fallback to cached dashboard data (Memorystore Redis, 1 hour stale)
   â”‚   â”œâ”€â”€ Read replicas in secondary region (eventual consistency, <5 min lag)
   â”‚   â””â”€â”€ Graceful degradation: Show "Data delayed" banner, serve cached results
   â””â”€â”€ SLA: 99.9% uptime (8.76 hrs downtime/year acceptable)

2. LLM API Rate Limits / Outages
   â”œâ”€â”€ Problem: Anthropic API rate limits hit during batch job (429 errors)
   â”œâ”€â”€ Mitigation:
   â”‚   â”œâ”€â”€ Exponential backoff with jitter (retry after 1s, 2s, 4s, 8s...)
   â”‚   â”œâ”€â”€ Circuit breaker: Pause batch job if error rate > 10%
   â”‚   â”œâ”€â”€ Fallback to cached categorizations (use yesterday's results)
   â”‚   â”œâ”€â”€ Multi-provider strategy: Fallback to OpenAI GPT-4 if Claude unavailable
   â”‚   â””â”€â”€ Queue overflow handling: Defer low-priority recategorizations
   â””â”€â”€ Monitoring: Alert on-call if LLM error rate > 5%

3. Data Pipeline Failure
   â”œâ”€â”€ Problem: Cloud Run job crashes during batch processing
   â”œâ”€â”€ Mitigation:
   â”‚   â”œâ”€â”€ Checkpointing: Store progress every 500 users processed
   â”‚   â”œâ”€â”€ Auto-restart from last checkpoint (don't reprocess 5,000 users)
   â”‚   â”œâ”€â”€ Idempotency: Safe to retry same user multiple times
   â”‚   â”œâ”€â”€ Dead letter queue: Users that consistently fail â†’ manual review
   â”‚   â””â”€â”€ Parallel sharding: Process 10 shards of 556 users concurrently
   â”‚       (if shard 3 fails, shards 1-2, 4-10 continue)
   â””â”€â”€ Alert: PagerDuty notification if batch job fails 3 times

4. Data Corruption / Bad LLM Outputs
   â”œâ”€â”€ Problem: LLM hallucinates and categorizes all users as "CEO"
   â”œâ”€â”€ Mitigation:
   â”‚   â”œâ”€â”€ Validation layer: Reject if confidence < 0.6 or category not in allowlist
   â”‚   â”œâ”€â”€ Anomaly detection: Alert if >20% of users change category in one batch
   â”‚   â”œâ”€â”€ Rollback mechanism: Store previous 3 categorization versions
   â”‚   â”œâ”€â”€ Canary deployment: Test new prompt on 1% of users before full rollout
   â”‚   â””â”€â”€ Human review queue: Flag suspicious categorizations for manager override
   â””â”€â”€ Recovery: Rollback to previous categorization version (stored in BigQuery)

5. Cost Overrun
   â”œâ”€â”€ Problem: LLM costs spike from $3k/month to $30k/month unexpectedly
   â”œâ”€â”€ Mitigation:
   â”‚   â”œâ”€â”€ Budget alerts: Alert at 80% and 100% of monthly budget
   â”‚   â”œâ”€â”€ Auto-throttling: Pause batch jobs if daily spend > 2Ã— expected
   â”‚   â”œâ”€â”€ Cost attribution: Track per-client spend (identify which client driving costs)
   â”‚   â””â”€â”€ Emergency shutdown: Kill switch to halt all LLM calls if needed
   â””â”€â”€ Root cause analysis: Review tracing logs to identify cost spike source
```

---

## ðŸ“Š Interview Talking Points: System Design

### Opening (2 min)

**Interviewer:** "Great analysis! Now how would you build a production system to deliver these insights?"

**You:** "I'd design an agentic workflow that runs at petabyte scale for Fortune 1000 clients. Let me break this down into 7 key areas: business requirements, agent architecture, data pipeline, RBAC, evaluation, tracing, and scale. Should I start with the agent design since that's the core deliverable, or would you prefer to discuss data pipeline first?"

### Agent Architecture (5 min)

"The Work Categorizer Agent uses LLM + tool calling rather than traditional ML. Here's why:

**Traditional ML approach:** Train k-means clustering, get 'User in cluster 3' - not explainable to CEOs.

**Agentic approach:**

1. Agent queries BigQuery for user's app access patterns (last 90 days)
2. LLM analyzes with few-shot prompting: 'Given GitHub: 250 accesses, Slack: 180, Jira: 90, what type of work does this user do?'
3. LLM responds: 'Software Engineer (Frontend) BECAUSE heavy GitHub + Jira + Figma usage. Confidence: 95%.'
4. Agent suggests optimizations: 'Reduce auth steps for dev tools, bundle GitHub+Jira+Slack, alert on excessive Slack switching. Potential savings: 3-5 hrs/week per engineer = $1.5M-2.5M/year for 200 engineers.'

The key advantage: explainability for CEOs, adaptability to new apps without retraining, and natural language output."

### Data Pipeline (5 min)

"At Parable scale, we're processing 1 billion SSO events per day across 50 Fortune 1000 clients:

**Ingestion:** SSO providers (Okta, Google) â†’ Pub/Sub â†’ Cloud Run â†’ BigQuery
**Storage:** BigQuery partitioned by date + tenant_id, clustered by actor_id
**Processing:** Nightly Cloud Run batch job categorizes 5,556 users/day
**Scale:** 365 TB/year in BigQuery, archived to Iceberg after 90 days

**Single-tenant architecture:** Each client gets isolated GCP project, VPC, KMS keys. No shared data, but shared parameterized pipeline code."

### RBAC & Privacy (3 min)

"Row-level security in BigQuery:

- CEOs see only org-wide aggregates (no individual names)
- Managers see direct reports only
- IT/Security has full access with audit trail
- Employees see only their own data

PII handling: Hash employee IDs, anonymize IP addresses, add differential privacy noise to small groups."

### Evaluation (3 min)

"Three-pronged validation:

1. **Ground truth:** Compare agent categories to HR job titles (Target: >80% accuracy)
2. **Explainability:** Show managers categorizations, survey 'Does this match reality?' (Target: >85% yes)
3. **A/B test:** Treatment group gets role-based optimizations, control group generic policies. Measure productivity improvement (Target: 10-15% gain)"

### Tracing (2 min)

"Every LLM call logged to BigQuery with full metadata: model, tokens, cost, latency, prompt version. This enables:

- Cost monitoring: Alert if daily LLM spend > $500
- Performance monitoring: Alert if p95 latency > 5 sec
- Quality monitoring: Track accuracy vs ground truth
- Prompt version control: A/B test new prompts before rollout"

### Scale & Cost (3 min)

"At 500k users:

- LLM costs: ~$3.3k/month (very cheap! $0.02 per categorization)
- BigQuery: ~$60k-110k/month (storage + queries)
- Total: ~$0.13-0.23 per user/month

Latency: CEO dashboards load in <3 sec via materialized views + caching

Cost optimization: Partition pruning, materialize common aggregations, archive old data to Iceberg"

---

## ðŸŽ¨ Whiteboard Diagram: Work Categorizer Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         CEO DASHBOARD                               â”‚
â”‚  "Engineering (45%): $2.1M/year opportunity in tool friction"       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†‘
                              â”‚ Query (< 3 sec)
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   BIGQUERY (Materialized Views)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚ work_        â”‚  â”‚ productivity_â”‚  â”‚ sso_events_  â”‚              â”‚
â”‚  â”‚ categories   â”‚  â”‚ metrics      â”‚  â”‚ raw          â”‚              â”‚
â”‚  â”‚ (500k users) â”‚  â”‚ (aggregated) â”‚  â”‚ (365 TB)     â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†‘                                        â†‘
         â”‚ Write categorizations                 â”‚ Ingest SSO events
         â”‚ (nightly batch)                       â”‚ (real-time stream)
         â†“                                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CLOUD RUN JOB             â”‚      â”‚  PUB/SUB + CLOUD RUN       â”‚
â”‚  "work-categorizer-batch"  â”‚      â”‚  "sso-event-ingestion"     â”‚
â”‚                            â”‚      â”‚                            â”‚
â”‚  For each user:            â”‚      â”‚  Parse, validate, enrich   â”‚
â”‚  1. Query app history      â”‚â—„â”€â”€â”€â”€â”€â”¤  SSO events from Okta/etc  â”‚
â”‚  2. Call Work Categorizer  â”‚      â”‚                            â”‚
â”‚     Agent (LLM)            â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚  3. Store result           â”‚                   â†‘
â”‚                            â”‚                   â”‚ Webhooks
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
         â†“                                       â”‚
         â”‚ 2 LLM calls per user          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â†“                               â”‚  SSO PROVIDERS   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚  (Okta, Google)  â”‚
â”‚  WORK CATEGORIZER AGENT    â”‚           â”‚                  â”‚
â”‚                            â”‚           â”‚  1B events/day   â”‚
â”‚  Tools:                    â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚  â€¢ query_user_app_history  â”‚
â”‚  â€¢ query_app_cooccurrence  â”‚
â”‚  â€¢ categorize (LLM)        â”‚
â”‚  â€¢ explain (LLM)           â”‚
â”‚  â€¢ suggest_optimizations   â”‚
â”‚                            â”‚
â”‚  Output: Category +        â”‚
â”‚          Confidence +      â”‚
â”‚          Reasoning +       â”‚
â”‚          Recommendations   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
         â”‚ All LLM calls traced
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TRACING & MONITORING      â”‚
â”‚  â€¢ Cost: $3.3k/month       â”‚
â”‚  â€¢ Latency: p95 < 3 sec    â”‚
â”‚  â€¢ Accuracy: >80%          â”‚
â”‚  â€¢ Alerts: Slack/PagerDuty â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ—£ï¸ Common Interview Questions & Answers

### Q: "How would this handle 100x scale growth?"

**A:** "Great question. Let's say we grow from 500k to 50M users:

**Bottlenecks:**

1. LLM costs: $3.3k/month â†’ $330k/month (still reasonable, 1/3 of BigQuery costs)
2. BigQuery queries: Need aggressive caching, may hit quotas
3. Batch processing: 5,556 users/day â†’ 555k users/day

**Solutions:**

1. **Sampling:** Categorize power users (>50 app accesses/month) weekly, casual users monthly
   - Reduces from 555k/day to ~150k/day (70% reduction)
2. **Incremental updates:** Only recategorize when significant pattern change detected
   - Further reduces to ~50k/day (saves $200k/month in LLM costs)
3. **Distributed processing:** Shard Cloud Run jobs by tenant_id, run in parallel
   - 100 shards Ã— 500 users each = 2 hours â†’ 20 minutes with parallelization
4. **Response caching:** For common app signatures, cache LLM responses
   - 'GitHub+Jira+Slack' always â†’ 'Engineering' (no LLM call needed)
   - Cache hit rate ~40% â†’ saves $120k/month
5. **BigQuery BI Engine:** Cache 1 TB of hot data in-memory for sub-second queries
   - Dashboard queries drop from $50k to $10k/month

**Cost at 50M users with optimizations:**

- LLM: $330k/month â†’ $110k/month (caching + sampling)
- BigQuery: $100k/month â†’ $150k/month (scales sub-linearly with caching)
- Infrastructure: $10k/month â†’ $40k/month (distributed compute)
- **Total: ~$300k/month = $3.60/year per user** (economies of scale!)

Much cheaper than current $0.13-0.23/month Ã— 12 = $1.56-2.76/year because of caching and sampling."

---

### Q: "What if the LLM hallucinates or gives wrong categories?"

**A:** "Excellent question. Three layers of defense:

**1. Validation layer (pre-LLM):**

- Require minimum data: â‰¥30 days of app accesses, â‰¥10 events
- Reject nonsensical inputs (e.g., user with 0 app accesses)

**2. Confidence thresholding (post-LLM):**

- LLM must output confidence score (0-1)
- If confidence < 0.6 â†’ flag as 'Uncertain, needs manual review'
- Show these to managers for validation

**3. Human-in-the-loop (ongoing):**

- Managers can override agent categories ('This is wrong, user is actually a PM not Eng')
- Use overrides as ground truth for model retraining (few-shot examples in prompt)
- A/B test prompt improvements before rolling out

**Safety mechanism:**

- Never make automated IT policy changes based solely on agent output
- Always surface to manager/IT for approval
- Treat agent as 'recommendation engine' not 'decision engine'

**Monitoring:**

- Track manager override rate (if > 30% â†’ prompt needs improvement)
- Track user feedback ('Was this categorization helpful?' 1-5 rating)
- Alert if accuracy drops below 75%"

---

### Q: "How do you handle data privacy regulations like GDPR?"

**A:** "Privacy-by-design architecture:

**1. Data minimization:**

- Only collect SSO logs necessary for categorization (app access, timestamp)
- Don't log app CONTENT (e.g., email body, Slack messages)

**2. Anonymization:**

- Hash employee IDs with tenant-specific salt
- CEO dashboards show aggregates only (no individual names)

**3. Access controls:**

- Row-level security in BigQuery (managers see only direct reports)
- Audit log every data access (who, what, when)

**4. Right-to-be-forgotten:**

- GDPR deletion request â†’ purge user from BigQuery + Iceberg within 30 days
- Recalculate aggregates without that user

**5. Differential privacy:**

- Add Laplace noise to small groups (< 10 users) to prevent re-identification
- k-anonymity: Every reported group has â‰¥ 10 users

**6. Data residency:**

- EU clients â†’ GCP europe-west1 region (GDPR-compliant)
- US clients â†’ GCP us-central1 region

**7. Consent:**

- Employee onboarding: 'Your work tool usage will be analyzed to improve productivity. Opt-out available.'
- Opt-out: User data excluded from analysis (but still logged for security/compliance)"

---

### Q: "Why agentic workflow instead of just training a classifier?"

**A:** "Great question - this is the core of Parable's value prop. Let me contrast:

**Traditional ML Classifier:**

```
Input: User app usage vector [GitHub: 0.4, Slack: 0.3, Jira: 0.15, ...]
Model: Random Forest / SVM / Neural Net
Output: Class label [0, 1, 2, 3] â†’ 'Cluster 3'
```

**Problems:**

1. **Not explainable:** Why is user in cluster 3? Black box.
2. **Not CEO-friendly:** 'Cluster 3' means nothing. Need data scientist to interpret.
3. **Requires retraining:** New app (Linear) added â†’ retrain entire model (weeks of work)
4. **No natural language:** Can't say 'User is Engineer BECAUSE...'
5. **Static:** Can't adapt to edge cases or ask follow-up questions

**Agentic Workflow:**

```
Input: User app usage metrics
Agent: LLM with tools (query, analyze, explain)
Output: "Software Engineer (Frontend/Full-stack). Confidence: 95%.
         Reasoning: Heavy GitHub usage (250 accesses) indicates active coding.
         Jira access (90) suggests agile workflow. Figma access (30) suggests
         frontend work. This pattern matches typical frontend engineer signature.
         Recommendation: Reduce auth steps for GitHub/Jira, bundle dev tools."
```

**Advantages:**

1. **Explainable:** LLM provides reasoning with citations to raw data
2. **CEO-friendly:** Natural language anyone can understand
3. **Adapts immediately:** New app â†’ just update prompt, no retraining
4. **Handles edge cases:** User with unusual app combo â†’ LLM can reason through it
5. **Actionable:** Provides recommendations, not just labels

**When would you use traditional ML?**

- High-frequency, low-latency predictions (< 100ms) â†’ ML model in memory
- Well-defined, narrow task with tons of labeled data â†’ supervised learning
- Cost-sensitive (millions of predictions/day) â†’ ML inference cheaper than LLM

**For Parable:**

- CEO-facing insights (explainability critical) â†’ Agentic
- Adaptability to new apps (no retraining) â†’ Agentic
- Modest scale (5,556 categorizations/day) â†’ LLM costs acceptable ($3.3k/month)"

---

### Q: "Why not implement the agent in TypeScript since that's Parable's primary language?"

**A:** "Great question - the job description does emphasize TypeScript. Here's how I'd approach it:

**Recommended Architecture:**

- **Orchestration layer: TypeScript** (Parable's primary language)

  - API endpoints (Express.js or Fastify)
  - Job scheduling (Cloud Scheduler â†’ TypeScript Cloud Run)
  - Business logic, data transformations
  - BigQuery queries (using @google-cloud/bigquery TypeScript client)

- **LLM agent layer: TypeScript OR Python** (both work well)
  - TypeScript option: Use Anthropic's TypeScript SDK + LangChain.js
    - Pros: Single language, easier for Parable team to maintain
    - Cons: LangChain.js less mature than Python version
  - Python option: Lightweight Python microservice just for agent logic
    - Pros: Rich ecosystem (LangChain, better prompt libraries)
    - Cons: Extra deployment complexity

**My recommendation for Parable:**
Go full TypeScript to match team skillset. Here's why:

1. **Team consistency:** Job description requires 'strong TypeScript skills'
2. **Maintainability:** One language = simpler codebase
3. **Anthropic SDK:** TypeScript SDK is production-ready
4. **Example TypeScript agent:**

```typescript
import Anthropic from "@anthropic-ai/sdk";

async function categorizeUser(userId: string): Promise<WorkCategory> {
  const client = new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY });

  // Tool: Query BigQuery for user app history
  const appHistory = await queryUserAppHistory(userId, 90);

  // LLM call with structured output
  const response = await client.messages.create({
    model: "claude-sonnet-4.5",
    max_tokens: 500,
    messages: [
      {
        role: "user",
        content: buildPrompt(appHistory) // Prompt template
      }
    ]
  });

  // Parse JSON response
  const category: WorkCategory = JSON.parse(response.content[0].text);

  // Tracing: Log to BigQuery
  await logLLMCall({
    userId,
    model: "claude-sonnet-4.5",
    promptTokens: response.usage.input_tokens,
    completionTokens: response.usage.output_tokens,
    cost: calculateCost(response.usage),
    latency: response.latencyMs,
    category: category.category,
    confidence: category.confidence
  });

  return category;
}
```

**This shows:**

- TypeScript can absolutely handle agentic workflows
- Maintains Parable's tech stack consistency
- Production-ready with tracing, error handling
- Team can maintain without Python expertise"

---

### Q: "Why nightly batch instead of real-time categorization?"

**A:** "Excellent question - this is a deliberate trade-off. Let me explain:

**Real-Time Categorization:**

```
Every SSO event â†’ Trigger categorization â†’ LLM call
Pros: Always up-to-date
Cons:
  - Expensive: 1 billion events/day Ã— $0.02 = $20M/day in LLM costs!
  - Noisy: Category fluctuates daily based on single app access
  - Latency: 2-second LLM call delays SSO login flow
  - Unnecessary: Work category doesn't change daily
```

**Nightly Batch Categorization:**

```
Aggregate 90 days â†’ Categorize once â†’ Cache result
Pros:
  - Cheap: 5,556 users/day Ã— $0.02 = $111/day (1,800x cheaper!)
  - Stable: Category based on 90-day trend, not daily noise
  - No latency impact: Doesn't block SSO login
  - Accurate: Long-term patterns more predictive than single day
Cons:
  - Stale: Up to 24 hours delay for category updates
  - Miss rapid changes: User promoted yesterday, category updates tomorrow
```

**Hybrid Approach (Best of Both Worlds):**

```
1. Nightly batch: Standard recategorization every 90 days
2. Real-time triggers for events:
   - Manager override: "This category is wrong" â†’ immediate recategorization
   - Anomaly detection: User accesses 5+ new apps in one day â†’ trigger review
   - New hire onboarding: First 30 days â†’ weekly categorization (more dynamic)
3. Incremental updates: Daily pattern change score (cheap SQL heuristic)
   - If score > threshold â†’ queue for next batch run
```

**For Parable's use case:**
Nightly batch with real-time triggers is optimal because:

- Work categories are stable (engineers don't become salespeople overnight)
- CEO dashboards show trends (weekly/monthly), not real-time
- Cost savings are massive ($111/day vs $20M/day)
- Can still handle rapid changes via trigger system"

---

### Q: "How would you roll this out to 50 existing Parable clients?"

**A:** "Great question - rolling out a new feature to Fortune 1000 clients requires careful phasing. Here's my approach:

**Phase 1: Single Pilot Client (Weeks 1-2)**

```
Goal: Validate core functionality with one friendly client
Client: Mid-size company (5k employees), tech-savvy IT team

Activities:
â”œâ”€â”€ Deploy work categorizer to pilot client's isolated GCP project
â”œâ”€â”€ Run categorization on all 5k employees
â”œâ”€â”€ Manual validation: IT team reviews 100 random categorizations
â”‚   â””â”€â”€ Target: >80% accuracy before proceeding
â”œâ”€â”€ Show CEO dashboard to client executives
â”‚   â””â”€â”€ Gather feedback: Is this useful? What's missing?
â”œâ”€â”€ Iterate on prompt based on feedback
â”‚   â””â”€â”€ Example: Client uses "Product Engineering" not "Engineering"
â”‚         â†’ Update prompt with client-specific terminology
â””â”€â”€ Measure: Latency, cost, accuracy, user feedback

Success criteria:
- >80% categorization accuracy
- <3 sec dashboard load time
- Positive feedback from client CEO
- No production incidents
```

**Phase 2: Beta Rollout (Weeks 3-4)**

```
Goal: Validate at scale with 5 diverse clients
Clients: Mix of industries (finance, healthcare, tech, retail, manufacturing)

Activities:
â”œâ”€â”€ Deploy to 5 clients (50k employees total)
â”œâ”€â”€ A/B test within each client:
â”‚   â”œâ”€â”€ 50% of employees: Get role-based optimizations
â”‚   â””â”€â”€ 50% control: Generic IT policies
â”œâ”€â”€ Measure productivity impact:
â”‚   â”œâ”€â”€ Self-reported productivity surveys (weekly)
â”‚   â”œâ”€â”€ Auth time reduction (from SSO logs)
â”‚   â”œâ”€â”€ Context switching reduction (time gaps between app accesses)
â”‚   â””â”€â”€ Manager satisfaction with recommendations
â”œâ”€â”€ Collect edge cases:
â”‚   â””â”€â”€ Example: Healthcare client has "Nurse" category not in our prompt
â”‚         â†’ Add healthcare-specific roles to prompt
â””â”€â”€ Cost monitoring: Ensure LLM costs align with projections

Success criteria:
- Treatment group shows 10-15% productivity improvement
- All clients report >4.0/5 satisfaction with insights
- No cost overruns (actual within 20% of projected)
- Prompt handles diverse industries without major customization
```

**Phase 3: Gradual Rollout (Weeks 5-12)**

```
Goal: Scale to all 50 clients (500k employees) with risk mitigation
Strategy: 10% per week (5 clients/week)

Week-by-week:
â”œâ”€â”€ Week 5: 10 clients (100k employees)
â”œâ”€â”€ Week 6: 15 clients (150k employees)
â”œâ”€â”€ Week 7: 20 clients (200k employees)
â”œâ”€â”€ ...
â””â”€â”€ Week 12: 50 clients (500k employees) âœ…

Risk mitigation:
â”œâ”€â”€ Gradual increases allow monitoring for issues
â”œâ”€â”€ Rollback capability: If week 6 shows problems, pause rollout
â”œâ”€â”€ Canary monitoring: Alert if any metric degrades
â”‚   â”œâ”€â”€ Accuracy drops below 75%
â”‚   â”œâ”€â”€ LLM costs spike >50% from projection
â”‚   â”œâ”€â”€ Dashboard latency >5 sec
â”‚   â””â”€â”€ Error rate >5%
â””â”€â”€ Client-by-client customization:
    â””â”€â”€ Some clients need industry-specific categories (add to prompt)

Parallel activities:
â”œâ”€â”€ Onboarding docs for each client IT team
â”œâ”€â”€ CEO dashboard training sessions
â”œâ”€â”€ Slack channel for client feedback
â””â”€â”€ Weekly status updates to Parable leadership
```

**Phase 4: Production Monitoring & Iteration (Week 13+)**

```
Goal: Continuous improvement based on production data

Activities:
â”œâ”€â”€ Aggregate metrics across all 50 clients:
â”‚   â”œâ”€â”€ Overall categorization accuracy (target: >80%)
â”‚   â”œâ”€â”€ Productivity improvement (target: 10-15%)
â”‚   â”œâ”€â”€ ROI delivered (target: $1M+ time savings in 6 months)
â”‚   â””â”€â”€ Client satisfaction (target: >90% renewal rate)
â”œâ”€â”€ Identify improvement opportunities:
â”‚   â””â”€â”€ Example: Accuracy for "Product Manager" category is 65%
â”‚         â†’ Add more PM-specific app patterns to prompt
â”œâ”€â”€ Prompt iteration:
â”‚   â”œâ”€â”€ A/B test new prompts with 10% of users
â”‚   â”œâ”€â”€ Roll out improvements if accuracy increases >5%
â”‚   â””â”€â”€ Track prompt version performance over time
â””â”€â”€ Feature expansion based on client requests:
    â””â”€â”€ Example: Client wants "Team collaboration score" in addition to categorization

Success criteria:
- 100% of clients live with no major incidents
- Demonstrable productivity gains (quantified ROI)
- Feature requests prioritized for next quarter
- System scales to projected costs and latency
```

**Key Principles:**

1. **Start small:** 1 client â†’ 5 clients â†’ 50 clients (de-risk)
2. **Measure everything:** Accuracy, cost, latency, productivity, satisfaction
3. **Rollback ready:** Can revert any client if issues arise
4. **Client-specific:** Allow customization (industry terminology, categories)
5. **Gradual scale:** 10% per week allows catching issues early

This phased approach ensures we deliver value to Fortune 1000 clients without risking their production environments."

---

_For tips and checklists, see INTERVIEW_TIPS_AND_CHECKLISTS.md_
